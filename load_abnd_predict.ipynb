{"cells": [{"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-12-08 21:25:48 INFO     Config File Loaded\n2020-12-08 21:25:48 INFO     Config File Sections ['SETTINGS', 'LIVE_OMS', 'DATA_COLLATION', 'CURATED_DATA', 'LOAD_AND_PREDICT']\n2020-12-08 21:25:48 INFO     Staging Bucket gs://aes-analytics-0002-curated/Outage_Restoration/Staging/IPL_Live_Master_Dataset.csv\n2020-12-08 21:25:48 INFO     OMS LIVE CURATED DATASET LOADED \n\n2020-12-08 21:25:48 INFO     No of NAs if any: False \n\n2020-12-08 21:25:48 INFO     ****QC Check**** \n\n2020-12-08 21:25:48 INFO     Shape of the DataFrame (1, 158) \n\n2020-12-08 21:25:48 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Marker_Location', 'Dispatch_Location', 'cldCvrMin', 'cldCvrAvg', 'cldCvrMax', 'dewPtMin', 'dewPtAvg', 'dewPtMax', 'feelsLikeMin', 'feelsLikeAvg', 'feelsLikeMax', 'heatIndexMin', 'heatIndexAvg', 'heatIndexMax', 'mslPresMin', 'mslPresAvg', 'mslPresMax', 'precip', 'precipProb', 'radSolarMin', 'radSolarAvg', 'radSolarMax', 'radSolarTot', 'relHumMin', 'relHumAvg', 'relHumMax', 'sfcPresMin', 'sfcPresAvg', 'sfcPresMax', 'snowfall', 'snowfallProb', 'spcHumMin', 'spcHumAvg', 'spcHumMax', 'tempMin', 'tempAvg', 'tempMax', 'windChillMin', 'windChillAvg', 'windChillMax', 'windDirAvg', 'windDir80mAvg', 'windDir100mAvg', 'windSpdMin', 'windSpdAvg', 'windSpdMax', 'windSpd80mMin', 'windSpd80mAvg', 'windSpd80mMax', 'windSpd100mMin', 'windSpd100mAvg', 'windSpd100mMax', 'wetBulbMin', 'wetBulbAvg', 'wetBulbMax', 'tempRange', 'windSpdRange', 'sfcPresRange', 'cldCvrRange', 'relHumRange', 'relHumRatio', 'sfcPresRatio', 'WIND_DIRECTION', 'weekend_flag', 'Date', 'Priority_Customer_Qty', 'Dis_From_Live_Centriod_div_Cust_qty', 'Priority_Dist_Customer_Qty', 'NO_OF_POWER_OUT_CLUE_PER_DAY', 'NO_OF_OPEN_DEVICE_CLUE_PER_DAY', 'NO_OF_IVR_CLUE_PER_DAY', 'NO_OF_ANIMAL_CAUSE_PER_DAY', 'NO_OF_WIRE_OCCURN_PER_DAY', 'Outages_in_last_1hr', 'Outages_in_last_2hr', 'Outages_in_last_3hr', 'Outages_in_last_4hr', 'Outages_in_last_5hr', 'Outages_in_last_6hr', 'Outages_in_last_7hr', 'Outages_in_last_8hr', 'Outages_in_last_9hr', 'Outages_in_last_10hr', 'DOWNSTREAM_CUST_QTY'] \n\n/opt/conda/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n2020-12-08 21:25:48 INFO     Dates for which strom profiles will be read: ['20201208'] \n\n2020-12-08 21:25:48 INFO     Location of Storm Profiles gs://aes-analytics-0001-curated/Outage_Restoration/Live_Data_Curation/Storm_Profiles/ \n\n2020-12-08 21:25:48 INFO     Pre-processing Storm Info Done \n\n2020-12-08 21:25:48 INFO     ****QC Check**** \n\n2020-12-08 21:25:48 INFO     Shape of the DataFrame (20, 3) \n\n2020-12-08 21:25:48 INFO     Columns present in the DataFrame: ['Date', 'Marker_Location', 'Cluster_ID'] \n\n2020-12-08 21:25:48 INFO     Cluster Profiles Added \n\n2020-12-08 21:25:48 INFO     ****QC Check**** \n\n2020-12-08 21:25:48 INFO     Shape of the DataFrame (1, 159) \n\n2020-12-08 21:25:48 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Marker_Location', 'Dispatch_Location', 'cldCvrMin', 'cldCvrAvg', 'cldCvrMax', 'dewPtMin', 'dewPtAvg', 'dewPtMax', 'feelsLikeMin', 'feelsLikeAvg', 'feelsLikeMax', 'heatIndexMin', 'heatIndexAvg', 'heatIndexMax', 'mslPresMin', 'mslPresAvg', 'mslPresMax', 'precip', 'precipProb', 'radSolarMin', 'radSolarAvg', 'radSolarMax', 'radSolarTot', 'relHumMin', 'relHumAvg', 'relHumMax', 'sfcPresMin', 'sfcPresAvg', 'sfcPresMax', 'snowfall', 'snowfallProb', 'spcHumMin', 'spcHumAvg', 'spcHumMax', 'tempMin', 'tempAvg', 'tempMax', 'windChillMin', 'windChillAvg', 'windChillMax', 'windDirAvg', 'windDir80mAvg', 'windDir100mAvg', 'windSpdMin', 'windSpdAvg', 'windSpdMax', 'windSpd80mMin', 'windSpd80mAvg', 'windSpd80mMax', 'windSpd100mMin', 'windSpd100mAvg', 'windSpd100mMax', 'wetBulbMin', 'wetBulbAvg', 'wetBulbMax', 'tempRange', 'windSpdRange', 'sfcPresRange', 'cldCvrRange', 'relHumRange', 'relHumRatio', 'sfcPresRatio', 'WIND_DIRECTION', 'weekend_flag', 'Date', 'Priority_Customer_Qty', 'Dis_From_Live_Centriod_div_Cust_qty', 'Priority_Dist_Customer_Qty', 'NO_OF_POWER_OUT_CLUE_PER_DAY', 'NO_OF_OPEN_DEVICE_CLUE_PER_DAY', 'NO_OF_IVR_CLUE_PER_DAY', 'NO_OF_ANIMAL_CAUSE_PER_DAY', 'NO_OF_WIRE_OCCURN_PER_DAY', 'Outages_in_last_1hr', 'Outages_in_last_2hr', 'Outages_in_last_3hr', 'Outages_in_last_4hr', 'Outages_in_last_5hr', 'Outages_in_last_6hr', 'Outages_in_last_7hr', 'Outages_in_last_8hr', 'Outages_in_last_9hr', 'Outages_in_last_10hr', 'DOWNSTREAM_CUST_QTY', 'Cluster_ID'] \n\n"}], "source": "'''\nAuthor: Mu Sigma\nUpdated: 09 Dec 2020\nVersion: 2\nTasks : Load hypertuned Random forest model to predict total time for restoration\nand provide ETR's dataset and provided 0002 anbalytics locations\n'''\n\n# standard library imports\nimport pickle\nimport logging\nfrom pytz import timezone\nimport datetime as dt\nfrom datetime import datetime, date, timedelta\nimport pandas as pd\nfrom pandas.io import gbq\nimport numpy as np\nfrom configparser import ConfigParser, ExtendedInterpolation\n\n# third party import\nimport gcsfs\n\n# Setup logs\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# read config file\nCONFIGPARSER = ConfigParser(interpolation=ExtendedInterpolation())\nCONFIGPARSER.read('/root/confignew0002.ini')\nlogging.info('Config File Loaded')\nlogging.info('Config File Sections %s', CONFIGPARSER.sections())\n\ndef QC_CHECK_SHAPE_AND_COLUMNS(df):\n    '''\n    Input - Dataframe with operations/addtion of features/columns or joins performed\n    Output - Log Info using shape of dataframe and columns present\n    '''\n    logging.info('****QC Check**** \\n')\n    logging.info('Shape of the DataFrame %s \\n', df.shape)\n    logging.info('Columns present in the DataFrame: %s \\n', list(df.columns))\n    return\n\n## **Read OMS Live Curated Dataset**\nBUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STAGING_BUCKET']\nlogging.info('Staging Bucket %s', BUCKET_NAME)\n\nDF_ADS_FINAL = pd.read_csv(BUCKET_NAME)\n\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^Unnamed')]\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^c0')]\n\nlogging.info('OMS LIVE CURATED DATASET LOADED \\n')\nlogging.info('No of NAs if any: %s \\n', DF_ADS_FINAL.isnull().values.any())\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n## **Read Storm Profiles Data**\nBUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STORM_PROFILE_BUCKET']\nBUCKET_NAME = 'gs://aes-analytics-0001-curated/Outage_Restoration/Live_Data_Curation'\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(\n    DF_ADS_FINAL['CREATION_DATETIME'], errors='coerce')\nDF_ADS_FINAL['Date'] = DF_ADS_FINAL['CREATION_DATETIME'].dt.date\n\nUNIQUE_DATES = DF_ADS_FINAL[['Date']]\nUNIQUE_DATES.drop_duplicates(subset=['Date'], keep='first', inplace=True)\nUNIQUE_DATES['Date'] = UNIQUE_DATES['Date'].apply(lambda x: x.strftime('%Y%m%d'))\nUNIQUE = UNIQUE_DATES['Date'].to_list()\n\nlogging.info('Dates for which strom profiles will be read: %s \\n', UNIQUE)\n\nSTORM_PROFILES_LOCATION = BUCKET_NAME + '/Storm_Profiles/'\nlogging.info('Location of Storm Profiles %s \\n', STORM_PROFILES_LOCATION)\nSTORM_PROFILES_FILES = []\n\nfor i in UNIQUE:\n    FILENAME = STORM_PROFILES_LOCATION + 'storm_profiles_{}.csv'.format(i)\n    STORM_PROFILES_FILES.append(pd.read_csv(FILENAME))\n\nSTORMPROFILES_DF = pd.concat(STORM_PROFILES_FILES)\nSTORMPROFILES_DF.reset_index(drop=True, inplace=True)\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('^Unnamed')]\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('_c0')]\nSTORMPROFILES_DF = STORMPROFILES_DF[['timestamp', 'Location', 'clusters']]\nSTORMPROFILES_DF.rename({'timestamp' : 'Date', 'Location' : 'Marker_Location',\n                         'clusters' : 'Cluster_ID'}, axis=1, inplace=True)\nlogging.info('Pre-processing Storm Info Done \\n')\nQC_CHECK_SHAPE_AND_COLUMNS(STORMPROFILES_DF)\n\ndef rename_storm_info(row):\n    \"\"\"\n    Input - Cluster Number\n    Output - Full description and name of the clsuter after profling\n    \"\"\"\n    cluster_profile = ''\n    if row == 'Cluster1':\n        cluster_profile = 'Hot Days with Sudden Rain'\n    if row == 'Cluster2':\n        cluster_profile = 'Strong Breeze with Sudden Rain'\n    if row == 'Cluster3':\n        cluster_profile = 'Thunderstorms'\n    if row == 'Cluster4':\n        cluster_profile = 'Chilly Day with Chances of Snow'\n    if row == 'Cluster5':\n        cluster_profile = 'Strong Chilled Breeze with Chances of Snow'\n    if row == 'Cluster6':\n        cluster_profile = 'Hot Days with Chance of Rain'\n    \n    return cluster_profile\n\nSTORMPROFILES_DF['Cluster_ID'] = STORMPROFILES_DF['Cluster_ID'].apply(rename_storm_info)\n\ndef remove_spaces(string):\n    '''\n    Input - Maker name with spaces\n    Output - Marker name without space\n    Example i/p, o/p - Marker 1, Marker1\n    '''\n    return string.replace(\" \", \"\")\n\n\nSTORMPROFILES_DF['Marker_Location'] = STORMPROFILES_DF.apply(lambda x: remove_spaces(x['Marker_Location']), axis=1)\n\n# merge storm profiles with final dataframe\nDF_ADS_FINAL['Date'] = pd.to_datetime(DF_ADS_FINAL['Date'])\nSTORMPROFILES_DF['Date'] = pd.to_datetime(STORMPROFILES_DF['Date'])\nDF_ADS_FINAL = DF_ADS_FINAL.merge(STORMPROFILES_DF, how='left',\n                                  left_on=['Date', 'Marker_Location'],\n                                  right_on=['Date', 'Marker_Location'])\n\n\nlogging.info('Cluster Profiles Added \\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cluster_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Chilly Day with Chances of Snow</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                        Cluster_ID\n0  Chilly Day with Chances of Snow"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "DF_ADS_FINAL[['Cluster_ID']]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "## **Load Hyper Tuned RF model**\nRF_MODEL = pd.read_pickle(CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_LOCATION'])\nlogging.info(\"Model Loaded \\n\")\n\nMODEL_FEATURES = CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_FEATURES']\nFEATURES_DF = pd.read_csv(MODEL_FEATURES)\n\nFEATURE_LIST = list(FEATURES_DF.Features_List)\nlogging.info('Features Loaded \\n')\nlogging.info('Name of the features present %s \\n', FEATURE_LIST)\n\n## **Feature Pre-Processing before it is sent to the Model**\nDF_ADS_FINAL_V1 = DF_ADS_FINAL.copy(deep=True)\n\nDF_ADS_FINAL_V1['POWER_OUT_CLUE_FLG_False'] = DF_ADS_FINAL_V1['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['ST_OCCURN_FLG_False'] = DF_ADS_FINAL_V1['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['WIRE_OCCURN_FLG_False'] = DF_ADS_FINAL_V1['WIRE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['FUSE_OCCURN_FLG_False'] = DF_ADS_FINAL_V1['FUSE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['ST_OCCURN_FLG_True'] = DF_ADS_FINAL_V1['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['PUBLIC_SAFETY_OCCURN_FLG_True'] = DF_ADS_FINAL_V1['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['NO_CAUSE_FLG_False'] = DF_ADS_FINAL_V1['NO_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['ANIMAL_CAUSE_FLG_True'] = DF_ADS_FINAL_V1['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['DAY_FLAG_True'] = DF_ADS_FINAL_V1['DAY_FLAG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['UG_CAUSE_FLG_False'] = DF_ADS_FINAL_V1['UG_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['POLE_CLUE_FLG_False'] = DF_ADS_FINAL_V1['POLE_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['TREE_CAUSE_FLG_True'] = DF_ADS_FINAL_V1['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['ANIMAL_CAUSE_FLG_False'] = DF_ADS_FINAL_V1['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['TREE_CAUSE_FLG_False'] = DF_ADS_FINAL_V1['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['PUBLIC_SAFETY_OCCURN_FLG_False'] = DF_ADS_FINAL_V1['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL_V1['POWER_OUT_CLUE_FLG_True'] = DF_ADS_FINAL_V1['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL_V1['CITY_NAM_NO_CITY'] = DF_ADS_FINAL_V1['CITY_NAM'].apply(\n    lambda row: 1 if (row is 'NO_CITY') else 0)\n\nlogging.info(\"Preprocessing Done \\n\")\n\nY_TEST_PRED = RF_MODEL.predict(DF_ADS_FINAL_V1[FEATURE_LIST])\nY_TEST_PRED = np.exp(Y_TEST_PRED)\nY_TEST_PRED = list(Y_TEST_PRED)\nlogging.info('Predicted Values Are %s', Y_TEST_PRED)\n\n\ndef business_layer_add_addtional_time(predicted_values):\n    '''\n    Input - Prediction of Outages in minutes\n    Output - If predicted  values are less than 1440 min \n    Add 45 mins to predictions, Else Add 360 mins to predictions\n    '''\n    new_pred_values = []\n    for i in range(len(predicted_values)):\n        if predicted_values[i] <= 1440:\n            new_pred_values.append(predicted_values[i] + 45)\n        elif predicted_values[i] > 1440:\n            new_pred_values.append(predicted_values[i] + 360)\n        else :\n            new_pred_values.append(predicted_values[i])\n        \n    return new_pred_values\n\nY_TEST_PRED = business_layer_add_addtional_time(Y_TEST_PRED)\nDF_ADS_FINAL['Predicted_TTR'] = Y_TEST_PRED\nlogging.info('Business Logic Added \\n')\nlogging.info('Predicted ETRs after business logic %s \\n', Y_TEST_PRED)\nlogging.info('Predicted ETRs added to final dataframe \\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\ndef created_predicted_etr(creation_datetime, time_in_minutes):\n    \"\"\"\n    This function calculates the ETR timestamp using creation datetime\n    and time for restoration in minutes\n\n    Function returns ETR timestamp\n\n    Args:\n        creation_datetime - Outage Creation Datetime\n\t\ttime_in_minutes - TTR in minutes\n    \"\"\"\n    newtime = creation_datetime + timedelta(minutes=time_in_minutes)\n    newtime = newtime.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n    return newtime\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(DF_ADS_FINAL['CREATION_DATETIME'])\nDF_ADS_FINAL['Restoration_Period'] = round(DF_ADS_FINAL['Predicted_TTR'], 0)\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL.apply(\n    lambda row: created_predicted_etr(row['CREATION_DATETIME'], row['Predicted_TTR']), axis=1)\nDF_ADS_FINAL['Predicted_ETR'] = pd.to_datetime(DF_ADS_FINAL['Predicted_ETR'])\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].dt.round('10min')\nDF_ADS_FINAL['CREATION_DATETIME'] = DF_ADS_FINAL['CREATION_DATETIME'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\n\nlogging.info('Final ETRs Created \\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n## **Final Pre-processing to Write Outputs in correct Format**\nDF_ADS_FINAL = DF_ADS_FINAL[['OUTAGE_ID', 'INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID',\n                             'DNI_EQUIP_TYPE', 'CREATION_DATETIME', 'Predicted_ETR',\n                             'Restoration_Period', 'Cluster_ID']]\n\nDF_ADS_FINAL.rename({'CREATION_DATETIME' : 'Creation_Time',\n                     'Predicted_ETR' : 'Estimated_Restoration_Time',\n                     'Restoration_Period' : 'ETR','Cluster_ID' : 'Weather_Profile'}, axis=1, inplace=True)\n\n\n## **Read and Add Insertion Time to Outages**\nDF_PRED = DF_ADS_FINAL.copy(deep=True)\nDF_PRED['Last_Updated'] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\nDF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n                    chunksize=None, reauth=False, if_exists='append', auth_local_webserver=False,\n                    table_schema=None, location=None, progress_bar=True, credentials=None)\n\nDF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_LIVE_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n                    chunksize=None, reauth=False, if_exists='replace', auth_local_webserver=False,\n                    table_schema=None, location=None, progress_bar=True, credentials=None)\n\nlogging.info('Prediction Live path %s', CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'])\nDF_ADS_FINAL.to_csv(CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'], index=False)\n\nYEAR_MONTH = datetime.now(timezone('US/Eastern')).strftime('%Y-%m')\nCURRENT_DATE = datetime.now(timezone('US/Eastern')).strftime('%Y-%m-%d')\nCURRENT_DATE_HOUR = datetime.now(timezone('US/Eastern')).strftime('%Y%m%d%H%M')\nlogging.info('Year Month in Eastern Time Zone %s', YEAR_MONTH)\nlogging.info('Current Month in Eastern Time Zone %s', CURRENT_DATE)\nlogging.info('Current Date & Hour in Eastern Time Zone %s \\n', CURRENT_DATE_HOUR)\n\nFILENAME = CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_BACKUP'] + '{}/{}/TTR_predictions_{}.csv'.format(YEAR_MONTH, CURRENT_DATE, CURRENT_DATE_HOUR)\nlogging.info('Backup Storage Predictions Storage Path: %s \\n', FILENAME)\n\nDF_ADS_FINAL.to_csv(FILENAME, index=False)\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}