{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## **Import Required Packages**"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "'''\nThis code is used to take live data from OMS (Outage Manament System)\nAnd clean, filter, add different features & create analytical dataset\nFinal CSV written in STAGING PATH present in config0002\n'''\n\nimport ast\nimport os\nimport math\nimport json\nimport logging\nimport warnings\nimport operator\nimport subprocess\nimport time\nimport datetime as dt\nfrom datetime import date, timedelta, datetime\nimport pandas as pd\nimport numpy as np\nfrom pandas.io import gbq\nfrom google.cloud import storage\n\nimport geopy.distance\nfrom configparser import ConfigParser, ExtendedInterpolation\n\n# Setup logs\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:41 INFO     Config File Loaded\n2020-11-27 12:04:41 INFO     Config File Sections ['SETTINGS', 'LIVE_OMS', 'DATA_COLLATION', 'CURATED_DATA', 'LOAD_AND_PREDICT']\n"}], "source": "# read config file\n\nCONFIGPARSER = ConfigParser(interpolation=ExtendedInterpolation())\nCONFIGPARSER.read('/root/confignew0002.ini')\nlogging.info('Config File Loaded')\nlogging.info('Config File Sections %s', CONFIGPARSER.sections())"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "def QC_CHECK_SHAPE_AND_COLUMNS(df):\n    '''\n    Input - Dataframe with operations/addtion of features/columns or joins performed\n    Output - Log Info using shape of dataframe and columns present\n    '''\n    logging.info('****QC Check****')\n    logging.info('\\n')\n    logging.info('Shape of the DataFrame %s', df.shape)\n    logging.info('\\n')\n    logging.info('Columns present in the DataFrame: %s', list(df.columns))\n    logging.info('\\n')\n    return"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Check all Live Files present in Bucket**"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# sleep for 2 mins to avoid missing batch raw oms files if they are late by seconds\n# time.sleep(120)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:44 INFO     Todays Date: 2020-11-27\n2020-11-27 12:04:44 INFO     BUCKET_NAME: aes-datahub-0002-raw\n2020-11-27 12:04:44 INFO     \n\n2020-11-27 12:04:44 INFO     LIVE INCIDENT TABLES: ['OMS/2020-11-27/INCIDENT_IPL_202011270000.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270030.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270100.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270130.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270200.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270230.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270300.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270330.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270400.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270430.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270501.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270530.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270600.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270630.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270700.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270730.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270800.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270830.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270900.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011270930.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011271000.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011271030.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011271100.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011271130.csv', 'OMS/2020-11-27/INCIDENT_IPL_202011271200.csv']\n2020-11-27 12:04:44 INFO     \n\n2020-11-27 12:04:44 INFO     LIVE INCIDENT DEVICE TABLES: ['OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270000.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270030.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270100.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270130.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270200.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270230.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270300.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270330.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270400.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270430.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270501.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270530.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270600.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270630.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270700.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270730.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270800.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270830.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270900.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011270930.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271000.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271030.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271100.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271130.csv', 'OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271200.csv']\n2020-11-27 12:04:44 INFO     \n\n2020-11-27 12:04:44 INFO     LIVE LOCATION TABLES: ['OMS/2020-11-27/LOCATION_IPL_202011270000.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270030.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270100.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270130.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270200.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270230.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270300.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270330.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270400.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270430.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270501.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270530.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270600.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270630.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270700.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270730.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270800.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270830.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270900.csv', 'OMS/2020-11-27/LOCATION_IPL_202011270930.csv', 'OMS/2020-11-27/LOCATION_IPL_202011271000.csv', 'OMS/2020-11-27/LOCATION_IPL_202011271030.csv', 'OMS/2020-11-27/LOCATION_IPL_202011271100.csv', 'OMS/2020-11-27/LOCATION_IPL_202011271130.csv', 'OMS/2020-11-27/LOCATION_IPL_202011271200.csv']\n"}], "source": "CURRENT_DATE = datetime.today().strftime('%Y-%m-%d')\nlogging.info('Todays Date: %s', CURRENT_DATE)\nCLIENT = storage.Client()\n\nBUCKET_NAME = CONFIGPARSER['SETTINGS']['RAW_BUCKET_NAME']\nlogging.info('BUCKET_NAME: %s', BUCKET_NAME)\nlogging.info('\\n')\n\nBUCKET = CLIENT.get_bucket(BUCKET_NAME)\n\nBLOBS = BUCKET.list_blobs(prefix='OMS/'+CURRENT_DATE)\nDIRLIST = []\n\nfor blob in BLOBS:\n    DIRLIST.append(str(blob.name))\n\n#string matching to read tables\n_MATCHING_INCIDENT = [s for s in DIRLIST if \"INCIDENT_IPL\" in s]\n_MATCHING_LIVE_INCIDENT = [s for s in _MATCHING_INCIDENT if \"HIS\" not in s]\nlogging.info('LIVE INCIDENT TABLES: %s',_MATCHING_LIVE_INCIDENT)\nlogging.info('\\n')\n\n_MATCHING_INCIDENT_DEVICE = [s for s in DIRLIST if \"INCIDENT_DEVICE_IPL\" in s]\n_MATCHING_LIVE_INCIDENT_DEVICE = [s for s in _MATCHING_INCIDENT_DEVICE if \"HIS\" not in s]\nlogging.info('LIVE INCIDENT DEVICE TABLES: %s', _MATCHING_LIVE_INCIDENT_DEVICE)\nlogging.info('\\n')\n\n_MATCHING_LOCATION = [s for s in DIRLIST if \"LOCATION_IPL\" in s]\n_MATCHING_LIVE_LOCATION = [s for s in _MATCHING_LOCATION if \"HIS\" not in s]\nlogging.info('LIVE LOCATION TABLES: %s', _MATCHING_LIVE_LOCATION)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Read Live Files in Buckets**"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:45 INFO     Raw Bucket Location gs://aes-datahub-0002-raw\n2020-11-27 12:04:45 INFO     \n\n2020-11-27 12:04:45 INFO     2020-11-27\n2020-11-27 12:04:45 INFO     \n\n2020-11-27 12:04:45 INFO     Live LIVE_INCIDENT_DEVICE_PATH: gs://aes-datahub-0002-raw/OMS/2020-11-27/INCIDENT_DEVICE_IPL_202011271200.csv\n2020-11-27 12:04:45 INFO     \n\n2020-11-27 12:04:45 INFO     Live LIVE_INCIDENT_PATH: gs://aes-datahub-0002-raw/OMS/2020-11-27/INCIDENT_IPL_202011271200.csv\n2020-11-27 12:04:45 INFO     \n\n2020-11-27 12:04:46 INFO     Live LIVE_LOCATION_PATH: gs://aes-datahub-0002-raw/OMS/2020-11-27/LOCATION_IPL_202011271200.csv\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     LAST FILE READ PATH: gs://aes-analytics-0002-curated/Outage_Restoration/Staging/Last_OMS_File.csv\n2020-11-27 12:04:46 INFO     No new input data files in OMS\n"}, {"ename": "Exception", "evalue": "No new input data files from OMS", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-dc874ab5ea8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No new input data files in OMS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No new input data files from OMS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mCURRENT_FILE_READ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIGPARSER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LIVE_OMS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OMS_LAST_FILE_READ_NAME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mException\u001b[0m: No new input data files from OMS"]}], "source": "RAW_BUCKET_LOCATION = CONFIGPARSER['SETTINGS']['RAW_BUCKET_LOCATION']\nlogging.info('Raw Bucket Location %s', RAW_BUCKET_LOCATION)\nlogging.info('\\n')\n\nlogging.info(CURRENT_DATE)\nlogging.info('\\n')\n\nLIVE_INCIDENT_DEVICE = pd.read_csv(os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT_DEVICE[-1]))\nlogging.info('Live LIVE_INCIDENT_DEVICE_PATH: %s', os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT_DEVICE[-1]))\nlogging.info('\\n')\n\nLIVE_INCIDENT = pd.read_csv(os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT[-1]))\nlogging.info('Live LIVE_INCIDENT_PATH: %s', os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT[-1]))\nlogging.info('\\n')\n\nLIVE_LOCATION = pd.read_csv(os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_LOCATION[-1]))\nlogging.info('Live LIVE_LOCATION_PATH: %s', os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_LOCATION[-1]))\nlogging.info('\\n')\n\nFILE_READ_LIST = [os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT_DEVICE[-1]),\n                  os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_INCIDENT[-1]),\n                  os.path.join(RAW_BUCKET_LOCATION, _MATCHING_LIVE_LOCATION[-1])]\n\nCURRENT_FILE_READ = pd.DataFrame({'Filepath' : FILE_READ_LIST})\n\nlogging.info('LAST FILE READ PATH: %s', CONFIGPARSER['LIVE_OMS']['OMS_LAST_FILE_READ_NAME'])\n\ntry:\n    LAST_FILE_READ = pd.read_csv(\n        CONFIGPARSER['LIVE_OMS']['OMS_LAST_FILE_READ_NAME'])\nexcept:\n    LAST_FILE_READ = pd.DataFrame()\n    CURRENT_FILE_READ.to_csv(\n        CONFIGPARSER['LIVE_OMS']['OMS_LAST_FILE_READ_NAME'], index=False)\n\nif LAST_FILE_READ.empty:\n    logging.info(\"New Files Path's have been stored\")\nelse:\n    if ((CURRENT_FILE_READ.Filepath[0] == LAST_FILE_READ.Filepath[0]) and (\n    CURRENT_FILE_READ.Filepath[1] == LAST_FILE_READ.Filepath[1]) and (\n        CURRENT_FILE_READ.Filepath[2] == LAST_FILE_READ.Filepath[2])):\n\n        logging.info('No new input data files in OMS')\n        raise Exception('No new input data files from OMS')\n\nCURRENT_FILE_READ.to_csv(CONFIGPARSER['LIVE_OMS']['OMS_LAST_FILE_READ_NAME'], index=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **QC checks**"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     Shape of Live Incident Device Table (126, 53)\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     Shape of Live Location Table (131, 69)\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     Shape of Live Incident Table (130, 41)\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     No of Distinct INCIDENT_ID in INCIDENT_DEVICE Table 126\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     No of Distinct INCIDENT_ID in LOCATION Table 130\n2020-11-27 12:04:46 INFO     \n\n2020-11-27 12:04:46 INFO     ****QC Check****\n2020-11-27 12:04:46 INFO     No of Distinct INCIDENT_ID in INCIDENT Table 130\n2020-11-27 12:04:46 INFO     \n\n"}], "source": "logging.info(\"****QC Check****\")\nlogging.info(\"Shape of Live Incident Device Table %s\", LIVE_INCIDENT_DEVICE.shape)\nlogging.info(\"\\n\")\n\nSHAPE = LIVE_INCIDENT_DEVICE.shape[0]\nif SHAPE == 0:\n    raise Exception('Live Incident device table contains 0 rows')\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"Shape of Live Location Table %s\", LIVE_LOCATION.shape)\nlogging.info(\"\\n\")\n\nSHAPE = LIVE_LOCATION.shape[0]\nif SHAPE == 0:\n    raise Exception('Live location table contains 0 rows')\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"Shape of Live Incident Table %s\", LIVE_INCIDENT.shape)\nlogging.info(\"\\n\")\n\nSHAPE = LIVE_INCIDENT.shape[0]\nif SHAPE == 0:\n    raise Exception('Live incident contains 0 rows')\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"No of Distinct INCIDENT_ID in INCIDENT_DEVICE Table %s\", LIVE_INCIDENT_DEVICE.INCIDENT_ID.nunique())\nlogging.info(\"\\n\")\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"No of Distinct INCIDENT_ID in LOCATION Table %s\", LIVE_LOCATION.INCIDENT_ID.nunique())\nlogging.info(\"\\n\")\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"No of Distinct INCIDENT_ID in INCIDENT Table %s\", LIVE_INCIDENT.INCIDENT_ID.nunique())\nlogging.info(\"\\n\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Merge Files and Perform Data QC checks**"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:48 INFO     ****QC Check****\n2020-11-27 12:04:48 INFO     INCIDENT_DEVICE table before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\n2020-11-27 12:04:48 INFO     126 126\n2020-11-27 12:04:48 INFO     \n\n2020-11-27 12:04:48 INFO     ****QC Check****\n2020-11-27 12:04:48 INFO     LOCATION table before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\n2020-11-27 12:04:48 INFO     131 131\n2020-11-27 12:04:48 INFO     \n\n2020-11-27 12:04:49 INFO     ****QC Check****\n2020-11-27 12:04:49 INFO     INICDENT_DEVICE, LOCATION table merged before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\n2020-11-27 12:04:49 INFO     126 126\n2020-11-27 12:04:49 INFO     \n\n"}], "source": "# merge INCIDENT_DEVICE_ID and LOCATION table\n\nDF_INCIDENT_DEVICE_ = LIVE_INCIDENT_DEVICE.copy(deep=True)\nDF_LOCATION_ = LIVE_LOCATION.copy(deep=True)\nDF_INCIDENT_ = LIVE_INCIDENT.copy(deep=True)\n\ndel LIVE_INCIDENT_DEVICE, LIVE_LOCATION, LIVE_INCIDENT\n\n# subset location tables to get required columns for analysis\nDF_LOCATION_SUBSET = DF_LOCATION_[['INCIDENT_ID', 'LOCATION_ID', 'MAJ_OTG_ID',\n                                   'CITY_NAM', 'OCCURN_CD', 'CAUSE_CD', 'ENERGIZED_DATETIME']]\n\n# data quality qc\nlogging.info(\"****QC Check****\")\nlogging.info(\"INCIDENT_DEVICE table before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\")\nlogging.info(\"%s %s\", len(DF_INCIDENT_DEVICE_[['INCIDENT_ID', 'LOCATION_ID']]),\n             len(DF_INCIDENT_DEVICE_[['INCIDENT_ID', 'LOCATION_ID']].drop_duplicates()))\nlogging.info(\"\\n\")\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"LOCATION table before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\")\nlogging.info(\"%s %s\", len(DF_LOCATION_SUBSET[['INCIDENT_ID', 'LOCATION_ID']]),\n             len(DF_LOCATION_SUBSET[['INCIDENT_ID', 'LOCATION_ID']].drop_duplicates()))\nlogging.info(\"\\n\")\n\nDF_INCIDENTDEVICELOCATION_ = pd.merge(DF_INCIDENT_DEVICE_, DF_LOCATION_SUBSET,\n                                      on=['INCIDENT_ID', 'LOCATION_ID'], how='left')\n\nlogging.info(\"****QC Check****\")\nlogging.info(\"INICDENT_DEVICE, LOCATION table merged before and after dropping duplicates at INCIDENT_ID, LOCATION_ID\")\nlogging.info(\"%s %s\", len(DF_INCIDENTDEVICELOCATION_[['INCIDENT_ID', 'LOCATION_ID']]),\n                      len(DF_INCIDENTDEVICELOCATION_[['INCIDENT_ID', 'LOCATION_ID']].drop_duplicates()))\nlogging.info(\"\\n\")\n\nSHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\nif SHAPE == 0:\n    raise Exception('Incident and device location merge contains 0 rows')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Apply Required Filters**"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:51 INFO     Filter for customer quantity greater than 0\n2020-11-27 12:04:51 INFO     ****QC Check****\n2020-11-27 12:04:51 INFO     Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2020-11-27 12:04:51 INFO     NumExpr defaulting to 8 threads.\n2020-11-27 12:04:51 INFO     Rows left after checking for INCIDENTS whose CUSTOMER QUANTITY IS > 0 (8, 58)\n2020-11-27 12:04:51 INFO     \n\n2020-11-27 12:04:51 INFO     Filter for equp_stn_no is not NCC or not null\n2020-11-27 12:04:51 INFO     ****QC Check****\n2020-11-27 12:04:51 INFO     Rows left after checking that EQUIP_STN_NO is not from <<NON CONNECTED CUSTOMERS>> (6, 58)\n2020-11-27 12:04:51 INFO     \n\n2020-11-27 12:04:51 INFO     Removing NAN from DNI_EQIP_TYPE, CICRT_ID, STRCTUR_NO\n2020-11-27 12:04:51 INFO     ****QC Check****\n2020-11-27 12:04:51 INFO     Rows left after checking CIRCT_ID is not 0 and not null, STRCTUR_NO is not null and DNI_EQIP_TYPE is not null (6, 58)\n2020-11-27 12:04:51 INFO     \n\n2020-11-27 12:04:51 INFO     Removing CLUE_CD which start with 0 but do not start with 00\n2020-11-27 12:04:51 INFO     ****QC Check**** ['29LB' '24TH' '24TR' '99PL']\n"}], "source": "# customer quantity greater than 0\nlogging.info('Filter for customer quantity greater than 0')\nlogging.info(\"****QC Check****\")\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[(DF_INCIDENTDEVICELOCATION_.DOWNSTREAM_CUST_QTY > 0)]\nlogging.info('Rows left after checking for INCIDENTS whose CUSTOMER QUANTITY IS > 0 %s', DF_INCIDENTDEVICELOCATION_.shape)\nlogging.info(\"\\n\")\n\nSHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\nif SHAPE == 0:\n    raise Exception('Incident and device location merge contains 0 rows after CUST_QTY filter')\n\n# equip_stn_no is not NCC and not null\nlogging.info('Filter for equp_stn_no is not NCC or not null')\nlogging.info(\"****QC Check****\")\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[(DF_INCIDENTDEVICELOCATION_.EQUIP_STN_NO != '<NCC>') &\n     (DF_INCIDENTDEVICELOCATION_.EQUIP_STN_NO.notnull())]\nlogging.info(\"Rows left after checking that EQUIP_STN_NO is not from <<NON CONNECTED CUSTOMERS>> %s\", DF_INCIDENTDEVICELOCATION_.shape)\nlogging.info(\"\\n\")\n\nSHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\nif SHAPE == 0:\n    raise Exception('Incident and device location merge contains 0 rows after EQUIP_STN_NO filter')\n\n# removing NAN from DNI_EQUIP_TYPE, CIRCT_ID, STRCTUR_NO\nlogging.info('Removing NAN from DNI_EQIP_TYPE, CICRT_ID, STRCTUR_NO')\nlogging.info(\"****QC Check****\")\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[(DF_INCIDENTDEVICELOCATION_.CIRCT_ID != 0)]\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[~DF_INCIDENTDEVICELOCATION_.CIRCT_ID.isnull()]\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[~DF_INCIDENTDEVICELOCATION_.STRCTUR_NO.isnull()]\nDF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[~DF_INCIDENTDEVICELOCATION_.DNI_EQUIP_TYPE.isnull()]\nlogging.info(\"Rows left after checking CIRCT_ID is not 0 and not null, STRCTUR_NO is not null and DNI_EQIP_TYPE is not null %s\", DF_INCIDENTDEVICELOCATION_.shape)\nlogging.info(\"\\n\")\n\nSHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\nif SHAPE == 0:\n    raise Exception('Incident and device location merge contains 0 rows after ID filter')\n\n# removing CLUE_CD which start with 0 but does not start with 00\nlogging.info('Removing CLUE_CD which start with 0 but do not start with 00')\nlogging.info(\"****QC Check**** %s\", DF_INCIDENTDEVICELOCATION_.CLUE_CD.unique())\n# DF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[(DF_INCIDENTDEVICELOCATION_.CLUE_CD.str[:1] == '0') &\n#                                                         (DF_INCIDENTDEVICELOCATION_.CLUE_CD.str[:2] != '00')]\n# DF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[DF_INCIDENTDEVICELOCATION_.CLUE_CD != '01']\n# logging.info(\"Rows left after filtering for CLUE CODES which start with 0 but do not start with 00 %s\", DF_INCIDENTDEVICELOCATION_.shape)\n# logging.info(\"\\n\")\n\n# SHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\n# if SHAPE == 0:\n#     raise Exception('Incident and device location merge contains 0 rows after Clue filter')\n\n# # removing occurence codes starting with cancel, found ok and duplicate\n# logging.info('Removing OCCURN_CD which have descriptions starting with CANCEL, FOUND OK or DUPLICATE')\n# logging.info(\"****QC Check****\")\n# OCCUR_REMOV = json.loads(CONFIGPARSER.get(\"LIVE_OMS\",\"OCCURN_REMOV\"))\n# DF_INCIDENTDEVICELOCATION_ = DF_INCIDENTDEVICELOCATION_[~(DF_INCIDENTDEVICELOCATION_.OCCURN_CD.isin(OCCUR_REMOV))]\n# logging.info(\"Rows left after removing OCCURN_CD which have descriptions starting with CANCEL, FOUND OK or DUPLICATE %s\", DF_INCIDENTDEVICELOCATION_.shape)\n# logging.info(\"\\n\")\n\n# SHAPE = DF_INCIDENTDEVICELOCATION_.shape[0]\n# if SHAPE == 0:\n#     raise Exception('ADS contains 0 rows after OCCURN_CD filter')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Aggregate Numerical Columns**"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:04:53 INFO     Numerical Dataset Created\n2020-11-27 12:04:53 INFO     \n\n2020-11-27 12:04:53 INFO     ****QC Check****\n2020-11-27 12:04:53 INFO     \n\n2020-11-27 12:04:53 INFO     Shape of the DataFrame (6, 14)\n2020-11-27 12:04:53 INFO     \n\n2020-11-27 12:04:53 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID']\n2020-11-27 12:04:53 INFO     \n\n"}], "source": "## START ADS CREATION FOR NUMERICAL COLUMNS AT INCIDENT LEVEL\n\n# Aggregate numerical columns at INCIDENT_ID level to keep all unique INCIDNET_ID's\nDF_NUMERICAL = DF_INCIDENTDEVICELOCATION_.groupby(['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], as_index=False).agg({'CALL_QTY' : 'sum',\n    'DOWNSTREAM_CUST_QTY' : 'sum', 'KVA_VAL' : 'max', 'DOWNSTREAM_KVA_VAL' : 'max', 'INCIDENT_DEVICE_ID' : 'max', 'CREATION_DATETIME' : 'min',\n    'SUBST_ID' : 'min', 'LOCATION_ID' : 'max', 'ENERGIZED_DATETIME' : 'max'})\n\nDF_NUMERICAL.rename(columns={'DOWNSTREAM_CUST_QTY' : 'CUST_QTY'}, inplace=True)\n\nDF_NUMERICAL['INCIDENT_ID'] = DF_NUMERICAL['INCIDENT_ID'].astype(np.int64)\nDF_NUMERICAL['CIRCT_ID'] = DF_NUMERICAL['CIRCT_ID'].astype(np.int64)\n\nDF_NUMERICAL['OUTAGE_ID'] = DF_NUMERICAL.apply(lambda x: '%s%s%s%s' % (x['INCIDENT_ID'],\n                                                                       x['STRCTUR_NO'],\n                                                                       x['CIRCT_ID'],\n                                                                       x['DNI_EQUIP_TYPE']),\n                                               axis=1)\n\nlogging.info('Numerical Dataset Created')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_NUMERICAL)\n\n\nSHAPE = DF_NUMERICAL.shape[0]\nif SHAPE == 0:\n    raise Exception('ADS contains 0 rows after OCCURN_CD filter')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Create Day and Night Flags**"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:02 INFO     Day and Night flags created\n2020-11-27 12:05:02 INFO     \n\n2020-11-27 12:05:02 INFO     ****QC Check****\n2020-11-27 12:05:02 INFO     \n\n2020-11-27 12:05:02 INFO     Shape of the DataFrame (6, 15)\n2020-11-27 12:05:02 INFO     \n\n2020-11-27 12:05:02 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG']\n2020-11-27 12:05:02 INFO     \n\n"}], "source": "# Create Day and Night Flags from CREATION_DATETIME columns\n\nDF_NUMERICAL['CREATION_DATETIME'] = pd.to_datetime(DF_NUMERICAL['CREATION_DATETIME'], errors='coerce')\nDF_NUMERICAL['ENERGIZED_DATETIME'] = pd.to_datetime(DF_NUMERICAL['ENERGIZED_DATETIME'], errors='coerce')\n\nDF_NUMERICAL['DAY_FLAG'] = DF_NUMERICAL.CREATION_DATETIME.dt.hour.apply(lambda x: 1 if ((x >= 6) & (x < 18)) else 0)\n\nlogging.info('Day and Night flags created')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_NUMERICAL)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **City, Priority Treatment**"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:04 INFO     CITY NAME TREATED & PRIORITY VALUES ADDED\n2020-11-27 12:05:04 INFO     \n\n2020-11-27 12:05:04 INFO     ****QC Check****\n2020-11-27 12:05:04 INFO     \n\n2020-11-27 12:05:04 INFO     Shape of the DataFrame (6, 5)\n2020-11-27 12:05:04 INFO     \n\n2020-11-27 12:05:04 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CITY_NAM']\n2020-11-27 12:05:04 INFO     \n\n"}], "source": "DF_INCIDENTDEVICELOCATION_['PRIORITY_VAL_1.0'] = DF_INCIDENTDEVICELOCATION_['PRIORITY_VAL'].apply(lambda x: 1 if x == 1 else 0)\nDF_INCIDENTDEVICELOCATION_['PRIORITY_VAL_2.0'] = DF_INCIDENTDEVICELOCATION_['PRIORITY_VAL'].apply(lambda x: 1 if x == 2 else 0)\nDF_INCIDENTDEVICELOCATION_['PRIORITY_VAL_3.0'] = DF_INCIDENTDEVICELOCATION_['PRIORITY_VAL'].apply(lambda x: 1 if x == 3 else 0)\nDF_INCIDENTDEVICELOCATION_['PRIORITY_VAL_5.0'] = DF_INCIDENTDEVICELOCATION_['PRIORITY_VAL'].apply(lambda x: 1 if x == 5 else 0)\n\nDF_INCIDENTDEVICELOCATION_.drop(['PRIORITY_VAL'], axis=1, inplace=True)\n\nDF_INCIDENTDEVICELOCATION_.CITY_NAM = DF_INCIDENTDEVICELOCATION_.CITY_NAM.apply(\n    lambda x: 'INDIANAPOLIS' if(str(x).find('INDIAN') != -1) else x)\nDF_INCIDENTDEVICELOCATION_.CITY_NAM = DF_INCIDENTDEVICELOCATION_.CITY_NAM.apply(\n    lambda x: 'NO_CITY' if(x != x) else x)\n\n# city treatment\ndef cat_city_treat(group):\n    '''\n    Input - Grouped CITY_NAME (multiple cities can be present)\n    Output - Single CITY_NAME\n    '''\n    if group.CITY_NAM.nunique() > 1:\n        x = group[group.CITY_NAM != 'NO_CITY'].CITY_NAM.unique()\n        group.CITY_NAM = x[0]\n        return group\n    else:\n        return group\n\nDF_TREATED = DF_INCIDENTDEVICELOCATION_[['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CITY_NAM']]\nDF_TREATED = DF_TREATED.groupby(['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], as_index=False).apply(cat_city_treat)\nDF_TREATED.drop_duplicates(subset=['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], ignore_index=True, inplace=True)\n\nlogging.info('CITY NAME TREATED & PRIORITY VALUES ADDED')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_TREATED)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Cause, Clue, Occurn Mapping**"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:08 INFO     Categorical Columns List ['POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0']\n2020-11-27 12:05:08 INFO     \n\n2020-11-27 12:05:08 INFO     FLAGS ADDED\n2020-11-27 12:05:08 INFO     \n\n2020-11-27 12:05:08 INFO     ****QC Check****\n2020-11-27 12:05:08 INFO     \n\n2020-11-27 12:05:08 INFO     Shape of the DataFrame (6, 67)\n2020-11-27 12:05:08 INFO     \n\n2020-11-27 12:05:08 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM']\n2020-11-27 12:05:08 INFO     \n\n"}], "source": "# cause, occurn, clue mapping files\nCLUEMAPPING = pd.read_csv(CONFIGPARSER['LIVE_OMS']['CLUE_MAPPING_CSV'])\nOCCURNMAPPING = pd.read_csv(CONFIGPARSER['LIVE_OMS']['OCCURN_MAPPING_CSV'])\nCAUSEMAPPING = pd.read_csv(CONFIGPARSER['LIVE_OMS']['CAUSEMAPPING_CSV'])\n\nDF_INCIDENTDEVICELOCATION_ = pd.merge(DF_INCIDENTDEVICELOCATION_, CLUEMAPPING, on=['CLUE_CD'], how='left')\nDF_INCIDENTDEVICELOCATION_ = pd.merge(DF_INCIDENTDEVICELOCATION_, OCCURNMAPPING, on=['OCCURN_CD'], how='left')\nDF_INCIDENTDEVICELOCATION_ = pd.merge(DF_INCIDENTDEVICELOCATION_, CAUSEMAPPING, on=['CAUSE_CD'], how='left')\n\nDF_INCIDENTDEVICELOCATION_[\"CLUE_DESC\"] = DF_INCIDENTDEVICELOCATION_[\"CLUE_DESC\"].astype(str)\nDF_INCIDENTDEVICELOCATION_[\"CAUSE_DESC\"] = DF_INCIDENTDEVICELOCATION_[\"CAUSE_DESC\"].astype(str)\nDF_INCIDENTDEVICELOCATION_[\"OCCURN_DESC\"] = DF_INCIDENTDEVICELOCATION_[\"OCCURN_DESC\"].astype(str)\n\n# segregation of clue code desc\nDF_INCIDENTDEVICELOCATION_['POLE_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('pole') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['PART_LIGHT_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('part lights') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['EMERGENCY_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('emergency') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['POWER_OUT_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('power out') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['TREE_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('tree') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WIRE_DOWN_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('wire down') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['IVR_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.lower().find('ivr') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['EQUIPMENT_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.find('EQUIPMENT') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['TRANSFORMER_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.find('TRANSFORMER') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['OPEN_DEVICE_CLUE_FLG'] = DF_INCIDENTDEVICELOCATION_.CLUE_DESC.apply(\n    lambda x: 1 if (x.find('OPEN DEVICE') != -1) else 0)\n\n\n# segration of cause desc\nDF_INCIDENTDEVICELOCATION_['CAUSE_DESC1'] = DF_INCIDENTDEVICELOCATION_[['CAUSE_DESC']].fillna('0')\nDF_INCIDENTDEVICELOCATION_['OH_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if((x.find('OH') != -1) | (x.find('O.H.') != -1)) else 0)\nDF_INCIDENTDEVICELOCATION_['UG_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if((x.find('UG') != -1) | (x.find('U.G.') != -1)) else 0)\nDF_INCIDENTDEVICELOCATION_['ANIMAL_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('ANIMAL') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('WEATHER') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER_COLD_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('COLD') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER_LIGHTNING_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('LIGHTNING') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER__SNOW_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('SNOW') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER__WIND_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('WIND') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER__HEAT_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('HEAT') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WEATHER__FLOOD_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('FLOOD') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['PUBLIC_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('PUBLIC') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['STREET_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('ST ') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['SUBSTATION_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('SUBSTATION') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['TREE_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('TREE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['MISCELLANEOUS_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('MISCELLANEOUS') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['CUST_REQUEST_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('CUSTOMER REQUEST') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['NO_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('NO CAUSE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['PLANNED_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('PLANNED WORK') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['NO_OUTAGE_CAUSE_FLG'] = DF_INCIDENTDEVICELOCATION_.CAUSE_DESC1.apply(\n    lambda x: 1 if(x.find('NO OUTAGE') != -1) else 0)\n\n\n# segration of OCCURN desc\nDF_INCIDENTDEVICELOCATION_['FUSE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if((x.find('FUSE') != -1) & (x.find('FUSE NOT') == -1)) else 0)\nDF_INCIDENTDEVICELOCATION_['CUST_EQUIP_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('CUSTOMER EQUIP') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['POLE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('POLE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['TRANSFORMER_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('TRANSFORMER') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['METER_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('METER') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['SERVICE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('SERVICE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['CABLE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('CABLE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['ST_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('ST') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['FIRE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('FIRE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['FOUND_OPEN_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if((x.find('FOUND OPEN') != -1) & (x.find('NOT FOUND OPEN') == -1)) else 0)\nDF_INCIDENTDEVICELOCATION_['PUBLIC_SAFETY_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('SAFETY') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['WIRE_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('WIRE') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['SWITCH_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('SWITCH') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['CUTOUT_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('CUTOUT') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['REGULATOR_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('REGULATOR') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['CAP_BANK_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('CAP BANK') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['OH_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('OH') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_['RECLOSER_OCCURN_FLG'] = DF_INCIDENTDEVICELOCATION_.OCCURN_DESC.apply(\n    lambda x: 1 if(x.find('RECLOSER') != -1) else 0)\nDF_INCIDENTDEVICELOCATION_.drop(columns=['CAUSE_DESC1'], inplace=True)\n\n# preprocessing to get flags at INCIDENT_ID level (LEVEL SPECIFIED)\nPRIORITY_LIST = list(DF_INCIDENTDEVICELOCATION_.filter(regex=(\"PRIORITY_VAL\"), axis=1).columns)\n\n# load categorical list from config files\nCAT_LIST = ast.literal_eval(CONFIGPARSER.get(\"LIVE_OMS\", \"CAT_LIST\"))\nCAT_LIST = list(CAT_LIST)\nCAT_LIST = CAT_LIST+PRIORITY_LIST\n\nlogging.info('Categorical Columns List %s', CAT_LIST)\nlogging.info('\\n')\n\nDF_INCIDENTDEVICELOCATION_CAT = DF_INCIDENTDEVICELOCATION_.groupby(['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], as_index=False)[CAT_LIST].agg('sum')\n\nDUMMY_COL = list(DF_INCIDENTDEVICELOCATION_CAT.columns)\nDUMMY_COL.remove('INCIDENT_ID')\nDUMMY_COL.remove('STRCTUR_NO')\nDUMMY_COL.remove('CIRCT_ID')\nDUMMY_COL.remove('DNI_EQUIP_TYPE')\n\nfor i in DUMMY_COL:\n    DF_INCIDENTDEVICELOCATION_CAT[i] = DF_INCIDENTDEVICELOCATION_CAT[i].apply(lambda x: 1 if x >= 1 else 0)\n\n\n# merge numercial and categorical columns to get ADS at INCIDENT_ID level (LEVEL SPECIFIED)\n\nDF_ADS = pd.merge(DF_NUMERICAL, DF_INCIDENTDEVICELOCATION_CAT, on=['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], how='left')\nDF_ADS = pd.merge(DF_ADS, DF_TREATED, on=['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE'], how='left')\n\nlogging.info(\"FLAGS ADDED\")\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)\n\nSHAPE = DF_ADS.shape[0]\nif SHAPE == 0:\n    raise Exception('ADS contains 0 rows')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Add cyclicity according to hour**"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:10 INFO     Hour cyclicity Added\n2020-11-27 12:05:10 INFO     \n\n2020-11-27 12:05:10 INFO     ****QC Check****\n2020-11-27 12:05:10 INFO     \n\n2020-11-27 12:05:10 INFO     Shape of the DataFrame (6, 69)\n2020-11-27 12:05:10 INFO     \n\n2020-11-27 12:05:10 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos']\n2020-11-27 12:05:10 INFO     \n\n"}], "source": "DF_ADS['CREATION_DATETIME'] = pd.to_datetime(DF_ADS['CREATION_DATETIME'], errors='coerce')\nDF_ADS['Hour'] = DF_ADS['CREATION_DATETIME'].dt.hour\nDF_ADS['Hour_Sin'] = np.sin(DF_ADS.Hour*(2.*np.pi/24))\nDF_ADS['Hour_Cos'] = np.cos(DF_ADS.Hour*(2.*np.pi/24))\nDF_ADS.drop(['Hour'], axis=1, inplace=True)\n\nlogging.info(\"Hour cyclicity Added\")\nlogging.info(\"\\n\")\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **X Y Co-ordinate Conversion**"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:12 INFO     Location treatment done\n2020-11-27 12:05:12 INFO     \n\n2020-11-27 12:05:12 INFO     ****QC Check****\n2020-11-27 12:05:12 INFO     \n\n2020-11-27 12:05:12 INFO     Shape of the DataFrame (6, 71)\n2020-11-27 12:05:12 INFO     \n\n2020-11-27 12:05:12 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG']\n2020-11-27 12:05:12 INFO     \n\n"}], "source": "def change_to_loc(df):\n    '''\n    Input - GEO_X_COORD, GEO_Y_COORD state plane co-ordinates\n    Output - Converted LAT, LONG coordinates of the geo_x and geo_y state plane values\n    '''\n    demnorthing = df.GEO_Y_COORD\n    demeasting = df.GEO_X_COORD\n    northing = float(demnorthing) * 0.3048\n    easting = float(demeasting) * 0.3048\n    om = (northing - 250000 + 4151863.7425) / 6367236.89768\n    fo = om + (math.sin(om) * math.cos(om)) * (0.005022893948 + 0.000029370625 * math.pow(math.cos(om), 2) +\n        0.000000235059 * math.pow(math.cos(om), 4) + 0.000000002181 * math.pow(math.cos(om), 6))\n    tf = math.sin(fo) / math.cos(fo)\n    nf2 = 0.00673949677548 * math.pow(math.cos(fo), 2)\n    rn = 0.9999666667 * 6378137 / math.pow((1 - 0.0066943800229034 * math.pow(math.sin(fo), 2)), 0.5)\n    q = (easting - 100000) / rn\n    b2 = -0.5 * tf * (1 + nf2)\n    b4 = -(1 / 12) * (5 + (3 * math.pow(tf, 2)) + (nf2 * (1 - 9 * math.pow(tf, 2)) - 4 * math.pow(nf2, 2)))\n    b6 = (1 / 360) * (61 + (90 * math.pow(tf, 2)) + (45 * math.pow(tf, 4)) +\n        (nf2 * (46 - (252 * math.pow(tf, 2)) - (90 * math.pow(tf, 4)))))\n    lat = fo + b2 * math.pow(q, 2) * (1 + math.pow(q, 2) * (b4 + b6 * math.pow(q, 2)))\n    b3 = -(1 / 6) * (1 + 2 * math.pow(tf, 2) + nf2)\n    b5 = (1 / 120) * (5 + 28 * math.pow(tf, 2) + 24 * math.pow(tf, 4) + nf2 * (6 + 8 * math.pow(tf, 2)))\n    b7 = -(1 / 5040) * (61 + 662 * math.pow(tf, 2) + 1320 * math.pow(tf, 4) + 720 * math.pow(tf, 6))\n    l = q * (1 + math.pow(q, 2) * (b3 + math.pow(q, 2) * (b5 + b7 * math.pow(q, 2))))\n    lon = 1.4951653925 - l / math.cos(fo)\n    coord = [(lat * 57.2957795131), (-1 * lon * 57.2957795131)]\n    return coord[0], coord[1]\n\nDF_LOCATION_['LAT'], DF_LOCATION_['LONG'] = zip(*DF_LOCATION_.apply(change_to_loc, axis=1))\n\n# subset from geo coordinates from location table\nDF_GEO_LOCATION = DF_LOCATION_[['LOCATION_ID', 'INCIDENT_ID', 'LAT', 'LONG']]\n\n# merge with ADS\nDF_ADS = pd.merge(DF_ADS, DF_GEO_LOCATION, on=['LOCATION_ID', 'INCIDENT_ID'], how='left')\n\nlogging.info('Location treatment done')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Add Zones Feature**"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:14 INFO     Zones Added\n2020-11-27 12:05:14 INFO     \n\n2020-11-27 12:05:14 INFO     No of unique zones present ['ZONE3' 'ZONE4']\n2020-11-27 12:05:14 INFO     \n\n2020-11-27 12:05:14 INFO     ****QC Check****\n2020-11-27 12:05:14 INFO     \n\n2020-11-27 12:05:14 INFO     Shape of the DataFrame (6, 72)\n2020-11-27 12:05:14 INFO     \n\n2020-11-27 12:05:14 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE']\n2020-11-27 12:05:14 INFO     \n\n"}], "source": "# function to add zone feature to the ads according to geo coordinates\ndef add_zone_feature(df):\n    '''\n    Input - dataframe with LAT, LONG columns\n    Output - ZONES which the LAT, LONG belong to\n    '''\n    center_lat = 39.7684\n    center_long = -86.1581\n    zone = ''\n\n    if float(df['LAT']) < center_lat:\n        if float(df['LONG']) < center_long:\n            zone = 'ZONE1'\n        else:\n            zone = 'ZONE2'\n    else:\n        if float(df['LONG']) < center_long:\n            zone = 'ZONE4'\n        else:\n            zone = 'ZONE3'\n\n    return zone\n\n\nDF_ADS['ZONE'] = DF_ADS.apply(add_zone_feature, axis=1)\nlogging.info('Zones Added')\nlogging.info('\\n')\nlogging.info(\"No of unique zones present %s\", DF_ADS['ZONE'].unique())\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Create User defined Insertion Time Column**"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:15 INFO     Insertion time flag added\n2020-11-27 12:05:15 INFO     \n\n2020-11-27 12:05:15 INFO     ****QC Check****\n2020-11-27 12:05:15 INFO     \n\n2020-11-27 12:05:15 INFO     Shape of the DataFrame (6, 73)\n2020-11-27 12:05:15 INFO     \n\n2020-11-27 12:05:15 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME']\n2020-11-27 12:05:15 INFO     \n\n"}], "source": "# create insertion time flag\n\nDF_ADS['INSERTION_TIME'] = datetime.today().strftime('%Y%m%d%H%M')\nDF_ADS['INSERTION_TIME'] = DF_ADS['INSERTION_TIME'].astype(np.int64)\nlogging.info('Insertion time flag added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Rank Subsequent Outages**"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:18 INFO     Ranked Subsequent Outages Added\n2020-11-27 12:05:18 INFO     \n\n2020-11-27 12:05:18 INFO     ****QC Check****\n2020-11-27 12:05:18 INFO     \n\n2020-11-27 12:05:18 INFO     Shape of the DataFrame (6, 74)\n2020-11-27 12:05:18 INFO     \n\n2020-11-27 12:05:18 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES']\n2020-11-27 12:05:18 INFO     \n\n"}], "source": "DF_ADS['Date'] = DF_ADS.CREATION_DATETIME.dt.date\nDF_ADS['RANK_SUBSEQUENT_OUTAGES'] = DF_ADS.groupby(['Date'], as_index=False)['CREATION_DATETIME'].rank(method='dense', ascending=True)\nDF_ADS.drop(['Date'], axis=1, inplace=True) \n\nlogging.info('Ranked Subsequent Outages Added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Prepare weather mapping columns**"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:19 INFO     Marker List ['Marker1', 'Marker2', 'Marker3', 'Marker4', 'Marker5', 'Marker6', 'Marker7', 'Marker8', 'Marker9', 'Marker10', 'Marker11', 'Marker12', 'Marker13', 'Marker14', 'Marker15', 'Marker16', 'Marker17', 'Marker18', 'Marker19', 'Marker20']\n2020-11-27 12:05:19 INFO     \n\n"}], "source": "LIST_COLUMNS = ['LAT', 'LONG']\nDF_ADS[LIST_COLUMNS] = DF_ADS[LIST_COLUMNS].apply(pd.to_numeric, errors='coerce')\n\n# forward filling of LAT, LONGS to avoid NA's in LAT, LONGS\nDF_ADS['LAT'] = DF_ADS['LAT'].ffill()\nDF_ADS['LONG'] = DF_ADS['LONG'].ffill()\n\nMARKER_LOCATION_CSV = pd.read_csv(CONFIGPARSER['LIVE_OMS']['MARKER_LOCATION_CSV'])\n\nMARKER_LOCATION_CSV = MARKER_LOCATION_CSV.loc[:, ~MARKER_LOCATION_CSV.columns.str.contains('^Unnamed')]\nMARKER_LOCATION_CSV = MARKER_LOCATION_CSV.loc[:, ~MARKER_LOCATION_CSV.columns.str.contains('_c0')]\n\nMARKER_LIST = list(MARKER_LOCATION_CSV['Marker'])\nlogging.info('Marker List %s', MARKER_LIST)\nlogging.info('\\n')\n\nMARKER_LOCATION_CSV = MARKER_LOCATION_CSV.set_index('Marker').T.to_dict('list')\n\nDF_ADS['Marker1_LAT'], DF_ADS['Marker1_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[0])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[0])[1])\nDF_ADS['Marker2_LAT'], DF_ADS['Marker2_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[1])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[1])[1])\nDF_ADS['Marker3_LAT'], DF_ADS['Marker3_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[2])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[2])[1])\nDF_ADS['Marker4_LAT'], DF_ADS['Marker4_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[3])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[3])[1])\nDF_ADS['Marker5_LAT'], DF_ADS['Marker5_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[4])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[4])[1])\nDF_ADS['Marker6_LAT'], DF_ADS['Marker6_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[5])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[5])[1])\nDF_ADS['Marker7_LAT'], DF_ADS['Marker7_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[6])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[6])[1])\nDF_ADS['Marker8_LAT'], DF_ADS['Marker8_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[7])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[7])[1])\nDF_ADS['Marker9_LAT'], DF_ADS['Marker9_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[8])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[8])[1])\nDF_ADS['Marker10_LAT'], DF_ADS['Marker10_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[9])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[9])[1])\nDF_ADS['Marker11_LAT'], DF_ADS['Marker11_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[10])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[10])[1])\nDF_ADS['Marker12_LAT'], DF_ADS['Marker12_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[11])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[11])[1])\nDF_ADS['Marker13_LAT'], DF_ADS['Marker13_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[12])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[12])[1])\nDF_ADS['Marker14_LAT'], DF_ADS['Marker14_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[13])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[13])[1])\nDF_ADS['Marker15_LAT'], DF_ADS['Marker15_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[14])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[14])[1])\nDF_ADS['Marker16_LAT'], DF_ADS['Marker16_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[15])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[15])[1])\nDF_ADS['Marker17_LAT'], DF_ADS['Marker17_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[16])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[16])[1])\nDF_ADS['Marker18_LAT'], DF_ADS['Marker18_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[17])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[17])[1])\nDF_ADS['Marker19_LAT'], DF_ADS['Marker19_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[18])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[18])[1])\nDF_ADS['Marker20_LAT'], DF_ADS['Marker20_LONG'] = (\n    MARKER_LOCATION_CSV.get(MARKER_LIST[19])[0], MARKER_LOCATION_CSV.get(MARKER_LIST[19])[1])"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:20 INFO     Marker Mapping Added\n2020-11-27 12:05:20 INFO     \n\n2020-11-27 12:05:20 INFO     ****QC Check****\n2020-11-27 12:05:20 INFO     \n\n2020-11-27 12:05:20 INFO     Shape of the DataFrame (6, 114)\n2020-11-27 12:05:20 INFO     \n\n2020-11-27 12:05:20 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Marker1_LAT', 'Marker1_LONG', 'Marker2_LAT', 'Marker2_LONG', 'Marker3_LAT', 'Marker3_LONG', 'Marker4_LAT', 'Marker4_LONG', 'Marker5_LAT', 'Marker5_LONG', 'Marker6_LAT', 'Marker6_LONG', 'Marker7_LAT', 'Marker7_LONG', 'Marker8_LAT', 'Marker8_LONG', 'Marker9_LAT', 'Marker9_LONG', 'Marker10_LAT', 'Marker10_LONG', 'Marker11_LAT', 'Marker11_LONG', 'Marker12_LAT', 'Marker12_LONG', 'Marker13_LAT', 'Marker13_LONG', 'Marker14_LAT', 'Marker14_LONG', 'Marker15_LAT', 'Marker15_LONG', 'Marker16_LAT', 'Marker16_LONG', 'Marker17_LAT', 'Marker17_LONG', 'Marker18_LAT', 'Marker18_LONG', 'Marker19_LAT', 'Marker19_LONG', 'Marker20_LAT', 'Marker20_LONG']\n2020-11-27 12:05:20 INFO     \n\n"}], "source": "logging.info('Marker Mapping Added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:21 INFO     Marker Mapping Done\n2020-11-27 12:05:21 INFO     \n\n2020-11-27 12:05:21 INFO     ****QC Check****\n2020-11-27 12:05:21 INFO     \n\n2020-11-27 12:05:21 INFO     Shape of the DataFrame (6, 76)\n2020-11-27 12:05:21 INFO     \n\n2020-11-27 12:05:21 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Min_Distance', 'Marker_Location']\n2020-11-27 12:05:21 INFO     \n\n"}], "source": "# calculate distance from 2 lat long\n\ndef check_nulls(func):\n    '''\n    Decorator to check Point values p1, p2 value is not None\n    And Valid Latitude and Logitude coordinates\n    '''\n    def inner(p1, p2):\n        if(math.isnan(p1[0]))|(math.isnan(p1[1]))|(math.isnan(p2[0]))|(math.isnan(p2[1])):\n            logging.info('Invalid Lat or Long %s, %s, %s, %s', p1[0], p1[1], p2[0], p2[1])\n            logging.info('\\n')\n            return\n\n        return func(p1, p2)\n    return inner\n\n@check_nulls\ndef haversine(p1, p2):\n    '''\n    Input - point1 and point2 in LAT, LONG\n    Output - Minimum diatance from marker\n    '''\n    R = 6371     # earth radius in km\n    p1 = [math.radians(v) for v in p1]\n    p2 = [math.radians(v) for v in p2]\n\n    d_lat = p2[0] - p1[0]\n    d_lng = p2[1] - p1[1]\n    a = math.pow(math.sin(d_lat / 2), 2) + math.cos(p1[0]) * math.cos(p2[0]) * math.pow(math.sin(d_lng / 2), 2)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n    return R * c   # returns distance between p1 and p2 in km\n\n\ndef minimum_distance(lat, long, marker1_lat, marker2_lat, marker3_lat,\n                     marker4_lat, marker5_lat, marker6_lat, marker7_lat,\n                     marker8_lat, marker9_lat, marker10_lat, marker11_lat,\n                     marker12_lat, marker13_lat, marker14_lat, marker15_lat,\n                     marker16_lat, marker17_lat, marker18_lat, marker19_lat,\n                     marker20_lat, marker1_long, marker2_long, marker3_long,\n                     marker4_long, marker5_long, marker6_long, marker7_long,\n                     marker8_long, marker9_long, marker10_long, marker11_long,\n                     marker12_long, marker13_long, marker14_long, marker15_long,\n                     marker16_long, marker17_long, marker18_long, marker19_long,\n                     marker20_long):\n    '''\n    Input - latitude, longitude of outages and different marker locations\n    Tasks - Calculate minimum distance of outage location from Marker Location\n    to determine which location's weather data should be used\n    Output - Minimum distance from Marker Location and index of Marker location\n    '''\n    dist1 = haversine((lat, long), (marker1_lat, marker1_long))\n    dist2 = haversine((lat, long), (marker2_lat, marker2_long))\n    dist3 = haversine((lat, long), (marker3_lat, marker3_long))\n    dist4 = haversine((lat, long), (marker4_lat, marker4_long))\n    dist5 = haversine((lat, long), (marker5_lat, marker5_long))\n    dist6 = haversine((lat, long), (marker6_lat, marker6_long))\n    dist7 = haversine((lat, long), (marker7_lat, marker7_long))\n    dist8 = haversine((lat, long), (marker8_lat, marker8_long))\n    dist9 = haversine((lat, long), (marker9_lat, marker9_long))\n    dist10 = haversine((lat, long), (marker10_lat, marker10_long))\n    dist11 = haversine((lat, long), (marker11_lat, marker11_long))\n    dist12 = haversine((lat, long), (marker12_lat, marker12_long))\n    dist13 = haversine((lat, long), (marker13_lat, marker13_long))\n    dist14 = haversine((lat, long), (marker14_lat, marker14_long))\n    dist15 = haversine((lat, long), (marker15_lat, marker15_long))\n    dist16 = haversine((lat, long), (marker16_lat, marker16_long))\n    dist17 = haversine((lat, long), (marker17_lat, marker17_long))\n    dist18 = haversine((lat, long), (marker18_lat, marker18_long))\n    dist19 = haversine((lat, long), (marker19_lat, marker19_long))\n    dist20 = haversine((lat, long), (marker20_lat, marker20_long))\n\n    dist_list = [dist1, dist2, dist3, dist4, dist5, dist6, dist7, dist8, dist9, dist10,\n                 dist11, dist12, dist13, dist14, dist15, dist16, dist17, dist18, dist19, dist20]\n\n    min_index, min_value = min(enumerate(dist_list), key=operator.itemgetter(1))\n\n    if(math.isnan(lat)) | (math.isnan(long)):\n        return None, None\n    else:\n        return min_value, min_index+1\n\nDF_ADS['Min_Distance'], DF_ADS['Marker_Location'] = zip(\n    *DF_ADS.apply(lambda row: minimum_distance(\n        row['LAT'], row['LONG'], row['Marker1_LAT'], row['Marker2_LAT'],\n        row['Marker3_LAT'], row['Marker4_LAT'], row['Marker5_LAT'],\n        row['Marker6_LAT'], row['Marker7_LAT'], row['Marker8_LAT'],\n        row['Marker9_LAT'], row['Marker10_LAT'], row['Marker11_LAT'],\n        row['Marker12_LAT'], row['Marker13_LAT'], row['Marker14_LAT'],\n        row['Marker15_LAT'], row['Marker16_LAT'], row['Marker17_LAT'],\n        row['Marker18_LAT'], row['Marker19_LAT'], row['Marker20_LAT'],\n        row['Marker1_LONG'], row['Marker2_LONG'], row['Marker3_LONG'],\n        row['Marker4_LONG'], row['Marker5_LONG'], row['Marker6_LONG'],\n        row['Marker7_LONG'], row['Marker8_LONG'], row['Marker9_LONG'],\n        row['Marker10_LONG'], row['Marker11_LONG'], row['Marker12_LONG'],\n        row['Marker13_LONG'], row['Marker14_LONG'], row['Marker15_LONG'],\n        row['Marker16_LONG'], row['Marker17_LONG'], row['Marker18_LONG'],\n        row['Marker19_LONG'], row['Marker20_LONG']), axis=1))\n\n\n\nDF_ADS.drop(['Marker1_LAT', 'Marker2_LAT', 'Marker3_LAT', 'Marker4_LAT', 'Marker5_LAT',\n             'Marker6_LAT', 'Marker7_LAT', 'Marker8_LAT', 'Marker9_LAT', 'Marker10_LAT',\n             'Marker11_LAT', 'Marker12_LAT', 'Marker13_LAT', 'Marker14_LAT', 'Marker15_LAT',\n             'Marker16_LAT', 'Marker17_LAT', 'Marker18_LAT', 'Marker19_LAT', 'Marker20_LAT',\n             'Marker1_LONG', 'Marker2_LONG', 'Marker3_LONG', 'Marker4_LONG', 'Marker5_LONG',\n             'Marker6_LONG', 'Marker7_LONG', 'Marker8_LONG', 'Marker9_LONG', 'Marker10_LONG',\n             'Marker11_LONG', 'Marker12_LONG', 'Marker13_LONG', 'Marker14_LONG', 'Marker15_LONG',\n             'Marker16_LONG', 'Marker17_LONG', 'Marker18_LONG', 'Marker19_LONG',\n             'Marker20_LONG'], axis=1, inplace=True)\n\nDF_ADS['Marker_Location'] = 'Marker '+DF_ADS['Marker_Location'].astype(str)\n\nlogging.info('Marker Mapping Done')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Add dispatch Area Location**"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "def cal_distance_from_dipatch_area(lat, long):\n    '''\n    Input - Latitude, Logitude of an outage locations\n    Output - Minimum distance from a dispatch location \n    and its index to identify dispatch location name\n    '''\n    if(math.isnan(lat)) | (math.isnan(long)):\n        return None, None\n    else:\n        coords1 = [lat, long]\n        dist_34 = geopy.distance.distance(coords1, [39.8802, -86.2324]).miles\n        dist_arl = geopy.distance.distance(coords1, [39.8802, -86.0854]).miles\n        dist_mill = geopy.distance.distance(coords1, [39.7880, -86.2296]).miles\n        dist_english = geopy.distance.distance(coords1, [39.7880, -86.0868]).miles\n        dist_wii = geopy.distance.distance(coords1, [39.7003, -86.2303]).miles\n        dist_south = geopy.distance.distance(coords1, [39.7003, -86.0834]).miles\n\n        dist_list = [dist_34, dist_arl, dist_mill, dist_english, dist_wii, dist_south]\n\n        min_index, min_value = min(enumerate(dist_list), key=operator.itemgetter(1))\n\n        return min_value, min_index+1\n\n\nDF_ADS['Min_Distance'], DF_ADS['Grid'] = zip(\n    *DF_ADS.apply(lambda row: cal_distance_from_dipatch_area(\n        row['LAT'], row['LONG']), axis=1))"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:24 INFO     Dispatch Location Added\n2020-11-27 12:05:24 INFO     \n\n2020-11-27 12:05:24 INFO     ****QC Check****\n2020-11-27 12:05:24 INFO     \n\n2020-11-27 12:05:24 INFO     Shape of the DataFrame (6, 76)\n2020-11-27 12:05:24 INFO     \n\n2020-11-27 12:05:24 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Marker_Location', 'Dispatch_Location']\n2020-11-27 12:05:24 INFO     \n\n"}], "source": "def map_grid_to_location(row):\n    '''\n    Input - Row numbers of the Dipatch Location\n    Tasks - Add dispatch area location for all outages which are present\n    Output - Name of the actual Dispatch Area\n    '''\n    value = ''\n    if row == 1:\n        value = '34th'\n    elif row == 2:\n        value = 'ARL.'\n    elif row == 3:\n        value = 'MILL'\n    elif row == 4:\n        value = 'ENGLISH'\n    elif row == 5:\n        value = 'W.I.'\n    elif row == 6:\n        value = 'SOUTH'\n    else:\n        value = 'NO_LOCATION'\n\n    return value\n\n\nDF_ADS['Dispatch_Location'] = DF_ADS.apply(lambda row: map_grid_to_location(row['Grid']), axis=1)\nDF_ADS.drop(['Min_Distance', 'Grid'], axis=1, inplace=True)\n\nlogging.info('Dispatch Location Added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **QC Check**"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset 1\n2020-11-27 12:05:25 INFO     Check Level of Analytical Dataset Created: Empty DataFrame\nColumns: []\nIndex: []\n2020-11-27 12:05:25 INFO     \n\n2020-11-27 12:05:25 INFO     No of NAs present if any: False\n2020-11-27 12:05:25 INFO     \n\n"}], "source": "def check_level(group):\n    '''\n    Input - Finally Created Analytical Dataframe\n    Tasks - Checks level of the table created\n    Output - 1 1 1 1 if the level of the table is correct\n    '''\n    logging.info('Check Level of Analytical Dataset %s', len(group))\n\nlogging.info('Check Level of Analytical Dataset Created: %s',\n              DF_ADS.groupby(['INCIDENT_ID', 'STRCTUR_NO',\n                              'CIRCT_ID', 'DNI_EQUIP_TYPE']).apply(check_level))\nlogging.info('\\n')\nlogging.info('No of NAs present if any: %s', DF_ADS.isnull().values.any())\nlogging.info('\\n')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Write table to OMS Live Mapped Dataset to Curated OMS**"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-24 10:23:39 INFO     LIVE OMS STAGING PATH gs://aes-analytics-0002-curated/Outage_Restoration/Live_Data_Curation/OMS/OMS_Live_Data.csv\n2020-11-24 10:23:39 INFO     \n\n"}], "source": "LIVE_OMS_STAGING_PATH = CONFIGPARSER['LIVE_OMS']['LIVE_OMS_STAGING_PATH']\nlogging.info('LIVE OMS STAGING PATH %s', LIVE_OMS_STAGING_PATH)\nlogging.info('\\n')\n\nDF_ADS.to_csv(LIVE_OMS_STAGING_PATH, index=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}