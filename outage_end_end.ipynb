{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/plain": "'\\nUpdated: 15 Nov 2020\\nTasks: Live data processing, weather data addition,\\npost processing and profiles addition, Regression run\\nSchedule: At the end of every 30 minutes\\nDescription: Scheduled DAG to process OMS data arriving at 30 min interval,\\nadding the weather data and predicting using RDS model objects\\nEnvironment: Composer-0002\\nRun-time environments: Pyspark,SparkR and python callable\\n'"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "\"\"\"\nUpdated: 15 Nov 2020\nTasks: Live data processing, weather data addition,\npost processing and profiles addition, Regression run\nSchedule: At the end of every 30 minutes\nDescription: Scheduled DAG to process OMS data arriving at 30 min interval,\nadding the weather data and predicting using RDS model objects\nEnvironment: Composer-0002\nRun-time environments: Pyspark,SparkR and python callable\n\"\"\""}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'airflow'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-d0a95d1eeded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataproc_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataProcPySparkOperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mairflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow'"]}], "source": "import logging\nlogging.basicConfig(level=logging.INFO)\nimport subprocess\nimport datetime\nfrom airflow.models import Variable\nfrom airflow.contrib.operators.dataproc_operator import (DataProcPySparkOperator)\nfrom airflow.models import DAG"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "INFO:root:Config file loaded to cluster\nINFO:root:\n\n"}], "source": "# copy config file from gcs to root\n\nBASHCOMMAND = 'gsutil cp gs://aes-analytics-0002-curated/Outage_Restoration/Live_Data_Curation/Config/confignew0002.ini /root/'\nPROCESS = subprocess.Popen(BASHCOMMAND.split(), stdout=subprocess.PIPE)\nOUTPUT, ERROR = PROCESS.communicate()\nlogging.info(\"Config file loaded to cluster\")\nlogging.info('\\n')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ===================Variables=================================\nENV = Variable.get(\"env\")\nlogging.info(ENV)\n\nJOB_NAME = 'outage_end_end'\nPROJECT = 'aes-datahub-'+ENV\nCOMPOSER_NAME = 'composer-'+ENV\nBUCKET = 'aes-analytics-0002-curated'\nCOMPOSER_BUCKET = 'us-east4-composer-0002-8d07c42c-bucket'\nDATAPROC_BUCKET = 'aes-datahub-0002-temp'\nRAW_BUCKET = 'aes-datahub-'+ENV+'-raw'\nCLUSTER_NAME = 'outage-python-cluster-0002'\nBQ_PROJECT = \"aes-analytics-0002\"\nBQ_DATASET = \"mds_outage_restoration\"\nBQ_TABLE_CHVG = \"IPL_Live_Input_Master_Dataset\"\nBQ_TABLE_FINAL = \"IPL_LIVE_PREDICTIONS\"\nBQ_TABLE_REPO = \"IPL_PREDICTIONS\"\nCLUSTER_NAME_R = 'outage-r-cluster-0002'\nEMAIL = ['musigma.bkumar.c@aes.com']\nBQ_DATASET_NAME = \"mds_outage_restoration\"\n\nOUTPUT_DATE = datetime.datetime.now().strftime(\"%Y%m%d\")\n\nYESTERDAY = datetime.datetime.combine(\n    datetime.datetime.today() - datetime.timedelta(1),\n    datetime.datetime.min.time())\nSTART_TIME = datetime.datetime(2020, 10, 27, 14, 00, 00)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# =================== DAG Arguments =================================\nDEFAULT_ARGS = {\n    'start_date': START_TIME,\n    'email_on_failure': True,\n    'EMAIL': ['musigma.bkumar.c@aes.com', 'ms.gkumar.c@aes.com', 'eric.nussbaumer@aes.com'],\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': datetime.timedelta(minutes=1),\n    # Consistent network configs for all tasks\n    'gcp_conn_id': 'google_cloud_default',\n    'subnetwork_uri': COMPOSER_NAME,\n    'internal_ip_only': True,\n    'region': 'us-east4',\n    'zone': 'us-east4-c',\n    'labels': {'resource-owner': 'datascience',\n               'financial-identifier': 'digital'}}"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n# =================== DAG Definition =================================\nwith DAG(\n        dag_id=JOB_NAME,\n        default_args=DEFAULT_ARGS,\n        schedule_interval='*/32 * * * *'\n) as dag:\n    OMS_LIVE_DATASET_PREPROCESSING = DataProcPySparkOperator(task_id='OMS_LIVE_DATA_COLLATION',\n                                                         main='gs://us-east4-composer-0002-8d07c4'\\\n                                                               '2c-bucket/data/'\\\n                                                               'Outage_restoration/IPL/'\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'Python_scripts/live_oms_'\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'preprocessing_pylint.py',\n                                                         arguments=None,\n                                                         archives=None,\n                                                         pyfiles=None,\n                                                         files=None,\n                                                         cluster_name='dp-outage-python-0002',\n                                                         dataproc_pyspark_properties=None,\n                                                         dataproc_pyspark_jars=None,\n                                                         gcp_conn_id='google_cloud_default',\n                                                         delegate_to=None,\n                                                         region='us-east4',\n                                                         job_error_states=['ERROR'],\n                                                         dag=dag,\n                                                         email_on_failure=None)\n\n    WEATHER_DATA_COLLATION = DataProcPySparkOperator(task_id='WEATHER_DATA_COLLATION',\n                                                     main='gs://us-east4-composer-0002-'\\\n                        \t\t\t\t\t\t\t       '8d07c42c-bucket/data/Outage_'\\\n                        \t\t\t\t\t\t\t\t   'restoration/IPL/Python_scripts/'\\\n                        \t\t\t\t\t\t\t\t   'weather_source_data_collation'\\\n                        \t    \t\t\t\t\t\t   '_pylint.py',\n                                                     arguments=None,\n                                                     archives=None,\n                                                     pyfiles=None,\n                                                     files=None,\n                                                     cluster_name='dp-outage-python-0002',\n                                                     dataproc_pyspark_properties=None,\n                                                     dataproc_pyspark_jars=None,\n                                                     gcp_conn_id='google_cloud_default',\n                                                     delegate_to=None,\n                                                     region='us-east4',\n                                                     job_error_states=['ERROR'],\n                                                     dag=dag)\n\n    CURATED_DATASET_CREATION = DataProcPySparkOperator(task_id='CURATED_DATASET_CREATION',\n                                                       main='gs://us-east4-composer-0002-8d07c42c'\\\n                                                             '-bucket/data/Outage_restoration/IPL'\\\n                                                             '/Python_scripts/curated_dataset_'\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t         'creation_pylint.py',\n                                                       arguments=None,\n                                                       archives=None,\n                                                       pyfiles=None,\n                                                       files=None,\n                                                       cluster_name='dp-outage-python-0002',\n                                                       dataproc_pyspark_properties=None,\n                                                       dataproc_pyspark_jars=None,\n                                                       gcp_conn_id='google_cloud_default',\n                                                       delegate_to=None,\n                                                       region='us-east4',\n                                                       job_error_states=['ERROR'],\n                                                       dag=dag,\n                                                       email_on_failure=None)\n\n    REGRESSION_CODE_RUN = DataProcPySparkOperator(task_id='Regression_Run',\n                                                  main='gs://us-east4-composer-0002-8d07c42c-'\\\n                                                        'bucket/data/Outage_restoration/IPL/'\\\n                                                        'Python_scripts/load_predict_pylint.py',\n                                                  arguments=None,\n                                                  archives=None,\n                                                  pyfiles=None,\n                                                  files=None,\n                                                  cluster_name='dp-outage-python-0002',\n                                                  dataproc_pyspark_properties=None,\n                                                  dataproc_pyspark_jars=None,\n                                                  gcp_conn_id='google_cloud_default',\n                                                  delegate_to=None,\n                                                  region='us-east4',\n                                                  job_error_states=['ERROR'],\n                                                  dag=dag)\n# Create pipeline\nOMS_LIVE_DATASET_COLLATION >> WEATHER_DATA_COLLATION >> CURATED_DATASET_CREATION >> REGRESSION_CODE_RUN"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}