{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## **Import necessary packages**"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "\"\"\"\nLoad hypertuned Random forest model to predict total time\nfor restoration and provide ETR's to stakeholders\n\"\"\"\n\nimport pickle\nimport logging\nimport subprocess\nfrom pytz import timezone\nimport datetime as dt\nfrom datetime import datetime, date, timedelta\nimport pandas as pd\nfrom pandas.io import gbq\nimport numpy as np\nfrom configparser import ConfigParser, ExtendedInterpolation\nimport gcsfs\nimport re\n\n# Setup logs\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 16:51:33 INFO     Config File Loaded\n2020-11-27 16:51:33 INFO     Config File Sections ['SETTINGS', 'LIVE_OMS', 'DATA_COLLATION', 'CURATED_DATA', 'LOAD_AND_PREDICT']\n"}], "source": "# read config file\nCONFIGPARSER = ConfigParser(interpolation=ExtendedInterpolation())\nCONFIGPARSER.read('/root/confignew0002.ini')\nlogging.info('Config File Loaded')\nlogging.info('Config File Sections %s', CONFIGPARSER.sections())"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "def QC_CHECK_SHAPE_AND_COLUMNS(df):\n    '''\n    Input - Dataframe with operations/addtion of features/columns or joins performed\n    Output - Log Info using shape of dataframe and columns present\n    '''\n    logging.info('****QC Check****')\n    logging.info('\\n')\n    logging.info('Shape of the DataFrame %s', df.shape)\n    logging.info('\\n')\n    logging.info('Columns present in the DataFrame: %s', list(df.columns))\n    logging.info('\\n')\n    return"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Read OMS Live Curated Dataset**"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2020-11-27 16:51:40 INFO     Staging Bucket gs://aes-analytics-0002-curated/Outage_Restoration/Staging/IPL_Live_Master_Dataset.csv\n2020-11-27 16:51:40 INFO     OMS LIVE CURATED DATASET LOADED\n2020-11-27 16:51:40 INFO     \n\n2020-11-27 16:51:40 INFO     No of NAs if any: True\n2020-11-27 16:51:40 INFO     \n\n2020-11-27 16:51:40 INFO     ****QC Check****\n2020-11-27 16:51:40 INFO     \n\n2020-11-27 16:51:40 INFO     Shape of the DataFrame (2, 158)\n2020-11-27 16:51:40 INFO     \n\n2020-11-27 16:51:40 INFO     Columns present in the DataFrame: ['INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID', 'DNI_EQUIP_TYPE', 'CALL_QTY', 'CUST_QTY', 'KVA_VAL', 'DOWNSTREAM_KVA_VAL', 'INCIDENT_DEVICE_ID', 'CREATION_DATETIME', 'SUBST_ID', 'LOCATION_ID', 'ENERGIZED_DATETIME', 'OUTAGE_ID', 'DAY_FLAG', 'POLE_CLUE_FLG', 'PART_LIGHT_CLUE_FLG', 'EMERGENCY_CLUE_FLG', 'POWER_OUT_CLUE_FLG', 'TREE_CLUE_FLG', 'WIRE_DOWN_CLUE_FLG', 'IVR_CLUE_FLG', 'EQUIPMENT_CLUE_FLG', 'TRANSFORMER_CLUE_FLG', 'OPEN_DEVICE_CLUE_FLG', 'OH_CAUSE_FLG', 'UG_CAUSE_FLG', 'ANIMAL_CAUSE_FLG', 'WEATHER_CAUSE_FLG', 'WEATHER_COLD_CAUSE_FLG', 'WEATHER_LIGHTNING_CAUSE_FLG', 'WEATHER__SNOW_CAUSE_FLG', 'WEATHER__WIND_CAUSE_FLG', 'WEATHER__HEAT_CAUSE_FLG', 'WEATHER__FLOOD_CAUSE_FLG', 'PUBLIC_CAUSE_FLG', 'STREET_CAUSE_FLG', 'SUBSTATION_CAUSE_FLG', 'TREE_CAUSE_FLG', 'MISCELLANEOUS_CAUSE_FLG', 'CUST_REQUEST_CAUSE_FLG', 'NO_CAUSE_FLG', 'PLANNED_CAUSE_FLG', 'NO_OUTAGE_CAUSE_FLG', 'FUSE_OCCURN_FLG', 'CUST_EQUIP_OCCURN_FLG', 'POLE_OCCURN_FLG', 'TRANSFORMER_OCCURN_FLG', 'METER_OCCURN_FLG', 'SERVICE_OCCURN_FLG', 'CABLE_OCCURN_FLG', 'ST_OCCURN_FLG', 'FIRE_OCCURN_FLG', 'FOUND_OPEN_OCCURN_FLG', 'PUBLIC_SAFETY_OCCURN_FLG', 'WIRE_OCCURN_FLG', 'SWITCH_OCCURN_FLG', 'CUTOUT_OCCURN_FLG', 'REGULATOR_OCCURN_FLG', 'CAP_BANK_OCCURN_FLG', 'OH_OCCURN_FLG', 'RECLOSER_OCCURN_FLG', 'PRIORITY_VAL_1.0', 'PRIORITY_VAL_2.0', 'PRIORITY_VAL_3.0', 'PRIORITY_VAL_5.0', 'CITY_NAM', 'Hour_Sin', 'Hour_Cos', 'LAT', 'LONG', 'ZONE', 'INSERTION_TIME', 'RANK_SUBSEQUENT_OUTAGES', 'Marker_Location', 'Dispatch_Location', 'cldCvrMin', 'cldCvrAvg', 'cldCvrMax', 'dewPtMin', 'dewPtAvg', 'dewPtMax', 'feelsLikeMin', 'feelsLikeAvg', 'feelsLikeMax', 'heatIndexMin', 'heatIndexAvg', 'heatIndexMax', 'mslPresMin', 'mslPresAvg', 'mslPresMax', 'precip', 'precipProb', 'radSolarMin', 'radSolarAvg', 'radSolarMax', 'radSolarTot', 'relHumMin', 'relHumAvg', 'relHumMax', 'sfcPresMin', 'sfcPresAvg', 'sfcPresMax', 'snowfall', 'snowfallProb', 'spcHumMin', 'spcHumAvg', 'spcHumMax', 'tempMin', 'tempAvg', 'tempMax', 'windChillMin', 'windChillAvg', 'windChillMax', 'windDirAvg', 'windDir80mAvg', 'windDir100mAvg', 'windSpdMin', 'windSpdAvg', 'windSpdMax', 'windSpd80mMin', 'windSpd80mAvg', 'windSpd80mMax', 'windSpd100mMin', 'windSpd100mAvg', 'windSpd100mMax', 'wetBulbMin', 'wetBulbAvg', 'wetBulbMax', 'tempRange', 'windSpdRange', 'sfcPresRange', 'cldCvrRange', 'relHumRange', 'relHumRatio', 'sfcPresRatio', 'WIND_DIRECTION', 'weekend_flag', 'Date', 'Priority_Customer_Qty', 'Dis_From_Live_Centriod_div_Cust_qty', 'Priority_Dist_Customer_Qty', 'NO_OF_POWER_OUT_CLUE_PER_DAY', 'NO_OF_OPEN_DEVICE_CLUE_PER_DAY', 'NO_OF_IVR_CLUE_PER_DAY', 'NO_OF_ANIMAL_CAUSE_PER_DAY', 'NO_OF_WIRE_OCCURN_PER_DAY', 'Outages_in_last_1hr', 'Outages_in_last_2hr', 'Outages_in_last_3hr', 'Outages_in_last_4hr', 'Outages_in_last_5hr', 'Outages_in_last_6hr', 'Outages_in_last_7hr', 'Outages_in_last_8hr', 'Outages_in_last_9hr', 'Outages_in_last_10hr', 'DOWNSTREAM_CUST_QTY']\n2020-11-27 16:51:40 INFO     \n\n"}], "source": "BUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STAGING_BUCKET']\nlogging.info('Staging Bucket %s', BUCKET_NAME)\n\nDF_ADS_FINAL = pd.read_csv(BUCKET_NAME)\n\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^Unnamed')]\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^c0')]\n\nlogging.info('OMS LIVE CURATED DATASET LOADED')\nlogging.info('\\n')\nlogging.info('No of NAs if any: %s', DF_ADS_FINAL.isnull().values.any())\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n# gs://aes-analytics-0002-curated/Outage_Restoration/Staging/IPL_Live_Master_Dataset.csv"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>INCIDENT_ID</th>\n      <th>STRCTUR_NO</th>\n      <th>CIRCT_ID</th>\n      <th>DNI_EQUIP_TYPE</th>\n      <th>CALL_QTY</th>\n      <th>CUST_QTY</th>\n      <th>KVA_VAL</th>\n      <th>DOWNSTREAM_KVA_VAL</th>\n      <th>INCIDENT_DEVICE_ID</th>\n      <th>CREATION_DATETIME</th>\n      <th>...</th>\n      <th>Outages_in_last_2hr</th>\n      <th>Outages_in_last_3hr</th>\n      <th>Outages_in_last_4hr</th>\n      <th>Outages_in_last_5hr</th>\n      <th>Outages_in_last_6hr</th>\n      <th>Outages_in_last_7hr</th>\n      <th>Outages_in_last_8hr</th>\n      <th>Outages_in_last_9hr</th>\n      <th>Outages_in_last_10hr</th>\n      <th>DOWNSTREAM_CUST_QTY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001562970</td>\n      <td>397-B/144</td>\n      <td>1308</td>\n      <td>1TBOH</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2002774699</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2001562972</td>\n      <td>221BA/143</td>\n      <td>3156</td>\n      <td>1TPUG</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2002774701</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows \u00d7 158 columns</p>\n</div>", "text/plain": "   INCIDENT_ID STRCTUR_NO  CIRCT_ID DNI_EQUIP_TYPE  CALL_QTY  CUST_QTY  \\\n0   2001562970  397-B/144      1308          1TBOH         1         1   \n1   2001562972  221BA/143      3156          1TPUG         1         1   \n\n   KVA_VAL  DOWNSTREAM_KVA_VAL  INCIDENT_DEVICE_ID  CREATION_DATETIME  ...  \\\n0        0                   0          2002774699                NaN  ...   \n1        0                   0          2002774701                NaN  ...   \n\n   Outages_in_last_2hr  Outages_in_last_3hr Outages_in_last_4hr  \\\n0                    0                    0                   0   \n1                    0                    0                   0   \n\n  Outages_in_last_5hr  Outages_in_last_6hr  Outages_in_last_7hr  \\\n0                   0                    0                    0   \n1                   0                    0                    0   \n\n   Outages_in_last_8hr  Outages_in_last_9hr  Outages_in_last_10hr  \\\n0                    0                    0                     0   \n1                    0                    0                     0   \n\n   DOWNSTREAM_CUST_QTY  \n0                    1  \n1                    1  \n\n[2 rows x 158 columns]"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "DF_ADS_FINAL.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Read Storm Profiles Data**"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>INCIDENT_ID</th>\n      <th>STRCTUR_NO</th>\n      <th>CIRCT_ID</th>\n      <th>DNI_EQUIP_TYPE</th>\n      <th>CALL_QTY</th>\n      <th>CUST_QTY</th>\n      <th>KVA_VAL</th>\n      <th>DOWNSTREAM_KVA_VAL</th>\n      <th>INCIDENT_DEVICE_ID</th>\n      <th>CREATION_DATETIME</th>\n      <th>...</th>\n      <th>Outages_in_last_2hr</th>\n      <th>Outages_in_last_3hr</th>\n      <th>Outages_in_last_4hr</th>\n      <th>Outages_in_last_5hr</th>\n      <th>Outages_in_last_6hr</th>\n      <th>Outages_in_last_7hr</th>\n      <th>Outages_in_last_8hr</th>\n      <th>Outages_in_last_9hr</th>\n      <th>Outages_in_last_10hr</th>\n      <th>DOWNSTREAM_CUST_QTY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001562970</td>\n      <td>397-B/144</td>\n      <td>1308</td>\n      <td>1TBOH</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2002774699</td>\n      <td>NaT</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2001562972</td>\n      <td>221BA/143</td>\n      <td>3156</td>\n      <td>1TPUG</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2002774701</td>\n      <td>NaT</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows \u00d7 158 columns</p>\n</div>", "text/plain": "   INCIDENT_ID STRCTUR_NO  CIRCT_ID DNI_EQUIP_TYPE  CALL_QTY  CUST_QTY  \\\n0   2001562970  397-B/144      1308          1TBOH         1         1   \n1   2001562972  221BA/143      3156          1TPUG         1         1   \n\n   KVA_VAL  DOWNSTREAM_KVA_VAL  INCIDENT_DEVICE_ID CREATION_DATETIME  ...  \\\n0        0                   0          2002774699               NaT  ...   \n1        0                   0          2002774701               NaT  ...   \n\n   Outages_in_last_2hr  Outages_in_last_3hr Outages_in_last_4hr  \\\n0                    0                    0                   0   \n1                    0                    0                   0   \n\n  Outages_in_last_5hr  Outages_in_last_6hr  Outages_in_last_7hr  \\\n0                   0                    0                    0   \n1                   0                    0                    0   \n\n   Outages_in_last_8hr  Outages_in_last_9hr  Outages_in_last_10hr  \\\n0                    0                    0                     0   \n1                    0                    0                     0   \n\n   DOWNSTREAM_CUST_QTY  \n0                    1  \n1                    1  \n\n[2 rows x 158 columns]"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "DF_ADS_FINAL.h"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n"}, {"ename": "ValueError", "evalue": "NaTType does not support strftime", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "\u001b[0;32m<ipython-input-6-f0e8b0db2591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDF_ADS_FINAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mUNIQUE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4211\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4212\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n", "\u001b[0;32m<ipython-input-6-f0e8b0db2591>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDF_ADS_FINAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mUNIQUE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNIQUE_DATES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32mpandas/_libs/tslibs/nattype.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.nattype._make_error_func.f\u001b[0;34m()\u001b[0m\n", "\u001b[0;31mValueError\u001b[0m: NaTType does not support strftime"]}], "source": "BUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STORM_PROFILE_BUCKET']\nBUCKET_NAME = 'gs://aes-analytics-0001-curated/Outage_Restoration/Live_Data_Curation'\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(\n    DF_ADS_FINAL['CREATION_DATETIME'], errors='coerce')\nDF_ADS_FINAL['Date'] = DF_ADS_FINAL['CREATION_DATETIME'].dt.date\n\nUNIQUE_DATES = DF_ADS_FINAL[['Date']]\nUNIQUE_DATES.drop_duplicates(subset=['Date'], keep='first', inplace=True)\nUNIQUE_DATES['Date'] = UNIQUE_DATES['Date'].apply(lambda x: x.strftime('%Y%m%d'))\nUNIQUE = UNIQUE_DATES['Date'].to_list()\n\nlogging.info('Dates for which strom profiles will be read: %s', UNIQUE)\nlogging.info('\\n')\n\n\nSTORM_PROFILES_LOCATION = BUCKET_NAME + '/Storm_Profiles/'\nlogging.info('Location of Storm Profiles %s', STORM_PROFILES_LOCATION)\nlogging.info('\\n')\nSTORM_PROFILES_FILES = []\n\nfor i in UNIQUE:\n    FILENAME = STORM_PROFILES_LOCATION + 'storm_profiles_{}.csv'.format(i)\n    STORM_PROFILES_FILES.append(pd.read_csv(FILENAME))\n\nSTORMPROFILES_DF = pd.concat(STORM_PROFILES_FILES)\nSTORMPROFILES_DF.reset_index(drop=True, inplace=True)\n\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('^Unnamed')]\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('_c0')]\nSTORMPROFILES_DF = STORMPROFILES_DF[['timestamp', 'Location', 'clusters']]\n\nSTORMPROFILES_DF.rename({'timestamp' : 'Date', 'Location' : 'Marker_Location',\n                         'clusters' : 'Cluster_ID'}, axis=1, inplace=True)\n\nlogging.info('Pre-processing Storm Info Done')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(STORMPROFILES_DF)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "STORMPROFILES_DF"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['Date']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def rename_storm_info(row):\n    \"\"\"\n    Input - Cluster Number\n    Output - Full description and name of the cluster after profling\n    \"\"\"\n    cluster_profile = ''\n    if row == 'Cluster1':\n        cluster_profile = 'Hot Days with Sudden Rain'\n    if row == 'Cluster2':\n        cluster_profile = 'Strong Breeze with Sudden Rain'\n    if row == 'Cluster3':\n        cluster_profile = 'Thunderstorms'\n    if row == 'Cluster4':\n        cluster_profile = 'Chilly Day with Chances of Snow'\n    if row == 'Cluster5':\n        cluster_profile = 'Strong Chilled Breeze with Chances of Snow'\n    if row == 'Cluster6':\n        cluster_profile = 'Hot Days with Chance of Rain'\n    \n    return cluster_profile\n\nSTORMPROFILES_DF['Cluster_ID'] = STORMPROFILES_DF['Cluster_ID'].apply(rename_storm_info)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "STORMPROFILES_DF"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['Marker_Location']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['Date']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Making the Marker Location string matching \nfor i in range(len(DF_ADS_FINAL)):\n    if len(DF_ADS_FINAL['Marker_Location'][i].split())==1:\n        DF_ADS_FINAL['Marker_Location'][i] = re.sub('(\\d+(\\.\\d+)?)', r' \\1 ', DF_ADS_FINAL['Marker_Location'][i])\n        \nfor i in range(len(STORMPROFILES_DF)):\n    if len(STORMPROFILES_DF['Marker_Location'][i].split())==1:\n        STORMPROFILES_DF['Marker_Location'][i] = re.sub('(\\d+(\\.\\d+)?)', r' \\1 ', STORMPROFILES_DF['Marker_Location'][i])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# merge storm profiles with final dataframe\nDF_ADS_FINAL['Date'] = pd.to_datetime(DF_ADS_FINAL['Date'])\nSTORMPROFILES_DF['Date'] = pd.to_datetime(STORMPROFILES_DF['Date'])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL.isna().sum()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL.shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "STORMPROFILES_DF[['Date', 'Marker_Location']].dtypes"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "STORMPROFILES_DF['Marker_Location'][0]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['Marker_Location'][0]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "STORMPROFILES_DF['Marker_Location']  = STORMPROFILES_DF['Marker_Location'].replace(\" \", \"\")\nDF_ADS_FINAL['Marker_Location']  = DF_ADS_FINAL['Marker_Location'].replace(' ', \"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL[['Date', 'Marker_Location']][0]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\nDF_ADS_FINAL = DF_ADS_FINAL.merge(STORMPROFILES_DF, how='left',\n                                  left_on=['Date', 'Marker_Location'],\n                                  right_on=['Date', 'Marker_Location'])\n\n\nlogging.info('Cluster Profiles Added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Load Hyper Tuned RF model**"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "RF_MODEL = pd.read_pickle(CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_LOCATION'])\nlogging.info(\"Model Loaded\")\nlogging.info('\\n')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "MODEL_FEATURES = CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_FEATURES']\nFEATURES_DF = pd.read_csv(MODEL_FEATURES)\n\nFEATURE_LIST = list(FEATURES_DF.Features_List)\nlogging.info('Features Loaded')\nlogging.info('\\n')\nlogging.info('Name of the features present %s', FEATURE_LIST)\nlogging.info('\\n')"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Feature Pre-Processing before it is sent to the Model**"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['POWER_OUT_CLUE_FLG_False'] = DF_ADS_FINAL['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ST_OCCURN_FLG_False'] = DF_ADS_FINAL['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['WIRE_OCCURN_FLG_False'] = DF_ADS_FINAL['WIRE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['FUSE_OCCURN_FLG_False'] = DF_ADS_FINAL['FUSE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ST_OCCURN_FLG_True'] = DF_ADS_FINAL['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG_True'] = \\\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['NO_CAUSE_FLG_False'] = DF_ADS_FINAL['NO_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ANIMAL_CAUSE_FLG_True'] = DF_ADS_FINAL['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['DAY_FLAG_True'] = DF_ADS_FINAL['DAY_FLAG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['UG_CAUSE_FLG_False'] = DF_ADS_FINAL['UG_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['POLE_CLUE_FLG_False'] = DF_ADS_FINAL['POLE_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['TREE_CAUSE_FLG_True'] = DF_ADS_FINAL['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['ANIMAL_CAUSE_FLG_False'] = DF_ADS_FINAL['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['TREE_CAUSE_FLG_False'] = DF_ADS_FINAL['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG_False'] = \\\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['POWER_OUT_CLUE_FLG_True'] = DF_ADS_FINAL['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['CITY_NAM_NO_CITY'] = DF_ADS_FINAL['CITY_NAM'].apply(\n    lambda row: 1 if (row is 'NO_CITY') else 0)\n\nlogging.info(\"Preprocessing Done\")\n\nDF_ADS_FINAL_TEMP = DF_ADS_FINAL.copy(deep=True)\n\nDF_ADS_FINAL = DF_ADS_FINAL[FEATURE_LIST]\n\nY_TEST_PRED = RF_MODEL.predict(DF_ADS_FINAL)\nY_TEST_PRED = np.exp(Y_TEST_PRED)\nY_TEST_PRED = list(Y_TEST_PRED)\n\nlogging.info('Predicted Values Are %s', Y_TEST_PRED)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def business_layer_add_addtional_time(predicted_values):\n    '''\n    Input - Prediction of Outages in minutes\n    Output - If predicted  values are less than 1440 min \n    Add 45 mins to predictions, Else Add 360 mins to predictions\n    '''\n    new_pred_values = []\n    for i in range(len(predicted_values)):\n        if predicted_values[i] <= 1440:\n            new_pred_values.append(predicted_values[i] + 45)\n        elif predicted_values[i] > 1440:\n            new_pred_values.append(predicted_values[i] + 360)\n        else :\n            new_pred_values.append(predicted_values[i])\n        \n    return new_pred_values\n\nY_TEST_PRED = business_layer_add_addtional_time(Y_TEST_PRED)\nDF_ADS_FINAL['Predicted_TTR'] = Y_TEST_PRED\n\nlogging.info('Business Logic Added')\nlogging.info('\\n')\nlogging.info('Predicted ETRs after business logic %s', Y_TEST_PRED)\nlogging.info('\\n')\n\nlogging.info('Predicted ETRs added to final dataframe')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#DF_ADS_FINAL_TEMP = DF_ADS_FINAL.copy(deep=True)\n\n#DF_ADS_FINAL['CREATION_DATETIME'] = DF_ADS_FINAL_TEMP['CREATION_DATETIME'].copy(deep=True) \nDF_ADS_FINAL = pd.concat([DF_ADS_FINAL, DF_ADS_FINAL_TEMP[['CREATION_DATETIME', 'STRCTUR_NO', 'Cluster_ID',\n                                                          'CIRCT_ID', 'DNI_EQUIP_TYPE', 'OUTAGE_ID',\n                                                          'INCIDENT_ID' ]]], axis=1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def created_predicted_etr(creation_datetime, time_in_minutes):\n    \"\"\"\n    This function calculates the ETR timestamp using creation datetime\n    and time for restoration in minutes\n\n    Function returns ETR timestamp\n\n    Args:\n        creation_datetime - Outage Creation Datetime\n\t\ttime_in_minutes - TTR in minutes\n    \"\"\"\n    newtime = creation_datetime + timedelta(minutes=time_in_minutes)\n    newtime = newtime.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n    return newtime\n\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(DF_ADS_FINAL['CREATION_DATETIME'])\nDF_ADS_FINAL['Restoration_Period'] = round(DF_ADS_FINAL['Predicted_TTR'], 0)\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL.apply(\n    lambda row: created_predicted_etr(row['CREATION_DATETIME'], row['Predicted_TTR']), axis=1)\n\nDF_ADS_FINAL['Predicted_ETR'] = pd.to_datetime(DF_ADS_FINAL['Predicted_ETR'])\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].dt.round('10min')\n\nDF_ADS_FINAL['CREATION_DATETIME'] = DF_ADS_FINAL['CREATION_DATETIME'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\n\nlogging.info('Final ETRs Created')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Final Pre-processing to Write Outputs in correct Format**"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL = DF_ADS_FINAL[['OUTAGE_ID', 'INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID',\n                             'DNI_EQUIP_TYPE', 'CREATION_DATETIME', 'Predicted_ETR',\n                             'Restoration_Period', 'Cluster_ID']]\n\nDF_ADS_FINAL.rename({'CREATION_DATETIME' : 'Creation_Time',\n                     'Predicted_ETR' : 'Estimated_Restoration_Time',\n                     'Restoration_Period' : 'ETR','Cluster_ID' : 'Weather_Profile'}, axis=1, inplace=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# DF_ADS_FINAL['Last_Updated'] = datetime.now().strftime(format='%Y-%m-%d %H:%M')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# DF_ADS_FINAL.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL['Creation_Time'] = DF_ADS_FINAL['Creation_Time'].astype(str)\nDF_ADS_FINAL['Estimated_Restoration_Time'] = DF_ADS_FINAL['Estimated_Restoration_Time'].astype(str)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_ADS_FINAL.dtypes"}, {"cell_type": "markdown", "metadata": {}, "source": "## **Read and Add Insertion Time to Outages**"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "DF_PRED = DF_ADS_FINAL.copy(deep=True)\nDF_PRED['Last_Updated'] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\nDF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n                    chunksize=None, reauth=False, if_exists='append', auth_local_webserver=False,\n                    table_schema=None, location=None, progress_bar=True, credentials=None)\n\nDF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_LIVE_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n                    chunksize=None, reauth=False, if_exists='replace', auth_local_webserver=False,\n                    table_schema=None, location=None, progress_bar=True, credentials=None)\n\nlogging.info('Prediction Live path %s', CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'])\nDF_ADS_FINAL.to_csv(CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'], index=False)\n\nYEAR_MONTH = datetime.now(timezone('US/Eastern')).strftime('%Y-%m')\nCURRENT_DATE = datetime.now(timezone('US/Eastern')).strftime('%Y-%m-%d')\nCURRENT_DATE_HOUR = datetime.now(timezone('US/Eastern')).strftime('%Y%m%d%H%M')\nlogging.info('Year Month in Eastern Time Zone %s', YEAR_MONTH)\nlogging.info('Current Month in Eastern Time Zone %s', CURRENT_DATE)\nlogging.info('Current Date & Hour in Eastern Time Zone %s', CURRENT_DATE_HOUR)\nlogging.info('\\n')\n\nFILENAME = CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_BACKUP'] + '{}/{}/TTR_predictions_{}.csv'.format(YEAR_MONTH, CURRENT_DATE, CURRENT_DATE_HOUR)\nlogging.info('Backup Storage Predictions Storage Path: %s', FILENAME)\n\nDF_ADS_FINAL.to_csv(FILENAME, index=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# '''\n# Author: Mu Sigma\n# Updated: 26 Nov 2020\n# Version: 1.5\n# Tasks : Load hypertuned Random forest model to predict total time for restoration\n# and provide ETR's dataset and provided 0002 anbalytics locations\n# '''\n\n# # standard library imports\n# import pickle\n# import logging\n# from pytz import timezone\n# import datetime as dt\n# from datetime import datetime, date, timedelta\n# import pandas as pd\n# from pandas.io import gbq\n# import numpy as np\n# from configparser import ConfigParser, ExtendedInterpolation\n\n# # third party import\n# import gcsfs\n\n# Setup logs\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\n# read config file\nCONFIGPARSER = ConfigParser(interpolation=ExtendedInterpolation())\nCONFIGPARSER.read('/root/confignew0002.ini')\nlogging.info('Config File Loaded')\nlogging.info('Config File Sections %s', CONFIGPARSER.sections())\n\n\n\ndef QC_CHECK_SHAPE_AND_COLUMNS(df):\n    '''\n    Input - Dataframe with operations/addtion of features/columns or joins performed\n    Output - Log Info using shape of dataframe and columns present\n    '''\n    logging.info('****QC Check****')\n    logging.info('\\n')\n    logging.info('Shape of the DataFrame %s', df.shape)\n    logging.info('\\n')\n    logging.info('Columns present in the DataFrame: %s', list(df.columns))\n    logging.info('\\n')\n    return\n\n\n# ## **Read OMS Live Curated Dataset**\n\n\nBUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STAGING_BUCKET']\nlogging.info('Staging Bucket %s', BUCKET_NAME)\n\nDF_ADS_FINAL = pd.read_csv(BUCKET_NAME)\n\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^Unnamed')]\nDF_ADS_FINAL = DF_ADS_FINAL.loc[:, ~DF_ADS_FINAL.columns.str.contains('^c0')]\n\nlogging.info('OMS LIVE CURATED DATASET LOADED')\nlogging.info('\\n')\nlogging.info('No of NAs if any: %s', DF_ADS_FINAL.isnull().values.any())\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n# gs://aes-analytics-0002-curated/Outage_Restoration/Staging/IPL_Live_Master_Dataset.csv\n\nDF_ADS_FINAL.head()\n\n\n# ## **Read Storm Profiles Data**\n\n\nBUCKET_NAME = CONFIGPARSER['LOAD_AND_PREDICT']['STORM_PROFILE_BUCKET']\nBUCKET_NAME = 'gs://aes-analytics-0001-curated/Outage_Restoration/Live_Data_Curation'\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(\n    DF_ADS_FINAL['CREATION_DATETIME'], errors='coerce')\nDF_ADS_FINAL['Date'] = DF_ADS_FINAL['CREATION_DATETIME'].dt.date\n\nUNIQUE_DATES = DF_ADS_FINAL[['Date']]\nUNIQUE_DATES.drop_duplicates(subset=['Date'], keep='first', inplace=True)\nUNIQUE_DATES['Date'] = UNIQUE_DATES['Date'].apply(lambda x: x.strftime('%Y%m%d'))\nUNIQUE = UNIQUE_DATES['Date'].to_list()\n\nlogging.info('Dates for which strom profiles will be read: %s', UNIQUE)\nlogging.info('\\n')\n\n\nSTORM_PROFILES_LOCATION = BUCKET_NAME + '/Storm_Profiles/'\nlogging.info('Location of Storm Profiles %s', STORM_PROFILES_LOCATION)\nlogging.info('\\n')\nSTORM_PROFILES_FILES = []\n\nfor i in UNIQUE:\n    FILENAME = STORM_PROFILES_LOCATION + 'storm_profiles_{}.csv'.format(i)\n    STORM_PROFILES_FILES.append(pd.read_csv(FILENAME))\n\nSTORMPROFILES_DF = pd.concat(STORM_PROFILES_FILES)\nSTORMPROFILES_DF.reset_index(drop=True, inplace=True)\n\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('^Unnamed')]\nSTORMPROFILES_DF = STORMPROFILES_DF.loc[:, ~STORMPROFILES_DF.columns.str.contains('_c0')]\nSTORMPROFILES_DF = STORMPROFILES_DF[['timestamp', 'Location', 'clusters']]\n\nSTORMPROFILES_DF.rename({'timestamp' : 'Date', 'Location' : 'Marker_Location',\n                         'clusters' : 'Cluster_ID'}, axis=1, inplace=True)\n\nlogging.info('Pre-processing Storm Info Done')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(STORMPROFILES_DF)\n\n\n\ndef rename_storm_info(row):\n    \"\"\"\n    Input - Cluster Number\n    Output - Full description and name of the clsuter after profling\n    \"\"\"\n    cluster_profile = ''\n    if row == 'Cluster1':\n        cluster_profile = 'Hot Days with Sudden Rain'\n    if row == 'Cluster2':\n        cluster_profile = 'Strong Breeze with Sudden Rain'\n    if row == 'Cluster3':\n        cluster_profile = 'Thunderstorms'\n    if row == 'Cluster4':\n        cluster_profile = 'Chilly Day with Chances of Snow'\n    if row == 'Cluster5':\n        cluster_profile = 'Strong Chilled Breeze with Chances of Snow'\n    if row == 'Cluster6':\n        cluster_profile = 'Hot Days with Chance of Rain'\n    \n    return cluster_profile\n\nSTORMPROFILES_DF['Cluster_ID'] = STORMPROFILES_DF['Cluster_ID'].apply(rename_storm_info)\n\n# merge storm profiles with final dataframe\nDF_ADS_FINAL['Date'] = pd.to_datetime(DF_ADS_FINAL['Date'])\nSTORMPROFILES_DF['Date'] = pd.to_datetime(STORMPROFILES_DF['Date'])\nDF_ADS_FINAL = DF_ADS_FINAL.merge(STORMPROFILES_DF, how='left',\n                                  left_on=['Date', 'Marker_Location'],\n                                  right_on=['Date', 'Marker_Location'])\n\n\nlogging.info('Cluster Profiles Added')\nlogging.info('\\n')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n\n# ## **Load Hyper Tuned RF model**\n\n\nRF_MODEL = pd.read_pickle(CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_LOCATION'])\nlogging.info(\"Model Loaded\")\nlogging.info('\\n')\n\n\nMODEL_FEATURES = CONFIGPARSER['LOAD_AND_PREDICT']['MODEL_FEATURES']\nFEATURES_DF = pd.read_csv(MODEL_FEATURES)\n\nFEATURE_LIST = list(FEATURES_DF.Features_List)\nlogging.info('Features Loaded')\nlogging.info('\\n')\nlogging.info('Name of the features present %s', FEATURE_LIST)\nlogging.info('\\n')\n\n\n# ## **Feature Pre-Processing before it is sent to the Model**\n\n\nDF_ADS_FINAL['POWER_OUT_CLUE_FLG_False'] = DF_ADS_FINAL['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ST_OCCURN_FLG_False'] = DF_ADS_FINAL['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['WIRE_OCCURN_FLG_False'] = DF_ADS_FINAL['WIRE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['FUSE_OCCURN_FLG_False'] = DF_ADS_FINAL['FUSE_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ST_OCCURN_FLG_True'] = DF_ADS_FINAL['ST_OCCURN_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG_True'] = DF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['NO_CAUSE_FLG_False'] = DF_ADS_FINAL['NO_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['ANIMAL_CAUSE_FLG_True'] = DF_ADS_FINAL['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['DAY_FLAG_True'] = DF_ADS_FINAL['DAY_FLAG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['UG_CAUSE_FLG_False'] = DF_ADS_FINAL['UG_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['POLE_CLUE_FLG_False'] = DF_ADS_FINAL['POLE_CLUE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['TREE_CAUSE_FLG_True'] = DF_ADS_FINAL['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['ANIMAL_CAUSE_FLG_False'] = DF_ADS_FINAL['ANIMAL_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['TREE_CAUSE_FLG_False'] = DF_ADS_FINAL['TREE_CAUSE_FLG'].apply(\n    lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG_False'] = DF_ADS_FINAL['PUBLIC_SAFETY_OCCURN_FLG'].apply(lambda row: 1 if (row is False) else 0)\nDF_ADS_FINAL['POWER_OUT_CLUE_FLG_True'] = DF_ADS_FINAL['POWER_OUT_CLUE_FLG'].apply(\n    lambda row: 1 if (row is True) else 0)\nDF_ADS_FINAL['CITY_NAM_NO_CITY'] = DF_ADS_FINAL['CITY_NAM'].apply(\n    lambda row: 1 if (row is 'NO_CITY') else 0)\n\nlogging.info(\"Preprocessing Done\")\n\nDF_ADS_FINAL_TEMP = DF_ADS_FINAL.copy(deep=True)\n\nDF_ADS_FINAL = DF_ADS_FINAL[FEATURE_LIST]\n\nY_TEST_PRED = RF_MODEL.predict(DF_ADS_FINAL)\nY_TEST_PRED = np.exp(Y_TEST_PRED)\nY_TEST_PRED = list(Y_TEST_PRED)\n\nlogging.info('Predicted Values Are %s', Y_TEST_PRED)\n\n\ndef business_layer_add_addtional_time(predicted_values):\n    '''\n    Input - Prediction of Outages in minutes\n    Output - If predicted  values are less than 1440 min \n    Add 45 mins to predictions, Else Add 360 mins to predictions\n    '''\n    new_pred_values = []\n    for i in range(len(predicted_values)):\n        if predicted_values[i] <= 1440:\n            new_pred_values.append(predicted_values[i] + 45)\n        elif predicted_values[i] > 1440:\n            new_pred_values.append(predicted_values[i] + 360)\n        else :\n            new_pred_values.append(predicted_values[i])\n        \n    return new_pred_values\n\nY_TEST_PRED = business_layer_add_addtional_time(Y_TEST_PRED)\nDF_ADS_FINAL['Predicted_TTR'] = Y_TEST_PRED\n\nlogging.info('Business Logic Added')\nlogging.info('\\n')\nlogging.info('Predicted ETRs after business logic %s', Y_TEST_PRED)\nlogging.info('\\n')\n\nlogging.info('Predicted ETRs added to final dataframe')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n\n#DF_ADS_FINAL_TEMP = DF_ADS_FINAL.copy(deep=True)\n\n#DF_ADS_FINAL['CREATION_DATETIME'] = DF_ADS_FINAL_TEMP['CREATION_DATETIME'].copy(deep=True) \nDF_ADS_FINAL = pd.concat([DF_ADS_FINAL, DF_ADS_FINAL_TEMP[['CREATION_DATETIME', 'STRCTUR_NO', 'Cluster_ID',\n                                                          'CIRCT_ID', 'DNI_EQUIP_TYPE', 'OUTAGE_ID',\n                                                          'INCIDENT_ID' ]]], axis=1)\n\n\n\ndef created_predicted_etr(creation_datetime, time_in_minutes):\n    \"\"\"\n    This function calculates the ETR timestamp using creation datetime\n    and time for restoration in minutes\n\n    Function returns ETR timestamp\n\n    Args:\n        creation_datetime - Outage Creation Datetime\n\t\ttime_in_minutes - TTR in minutes\n    \"\"\"\n    newtime = creation_datetime + timedelta(minutes=time_in_minutes)\n    newtime = newtime.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n    return newtime\n\n\nDF_ADS_FINAL['CREATION_DATETIME'] = pd.to_datetime(DF_ADS_FINAL['CREATION_DATETIME'])\nDF_ADS_FINAL['Restoration_Period'] = round(DF_ADS_FINAL['Predicted_TTR'], 0)\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL.apply(\n    lambda row: created_predicted_etr(row['CREATION_DATETIME'], row['Predicted_TTR']), axis=1)\n\nDF_ADS_FINAL['Predicted_ETR'] = pd.to_datetime(DF_ADS_FINAL['Predicted_ETR'])\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].dt.round('10min')\n\nDF_ADS_FINAL['CREATION_DATETIME'] = DF_ADS_FINAL['CREATION_DATETIME'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\nDF_ADS_FINAL['Predicted_ETR'] = DF_ADS_FINAL['Predicted_ETR'].apply(\n    lambda row: row.strftime(\"%Y/%m/%d %H:%M:%S\"))\n\nlogging.info('Final ETRs Created')\nQC_CHECK_SHAPE_AND_COLUMNS(DF_ADS_FINAL)\n\n\n# ## **Final Pre-processing to Write Outputs in correct Format**\n\n\nDF_ADS_FINAL = DF_ADS_FINAL[['OUTAGE_ID', 'INCIDENT_ID', 'STRCTUR_NO', 'CIRCT_ID',\n                             'DNI_EQUIP_TYPE', 'CREATION_DATETIME', 'Predicted_ETR',\n                             'Restoration_Period', 'Cluster_ID']]\n\nDF_ADS_FINAL.rename({'CREATION_DATETIME' : 'Creation_Time',\n                     'Predicted_ETR' : 'Estimated_Restoration_Time',\n                     'Restoration_Period' : 'ETR','Cluster_ID' : 'Weather_Profile'}, axis=1, inplace=True)\n\nDF_ADS_FINAL.head()\n# ## **Read and Add Insertion Time to Outages**\n\n\n# DF_PRED = DF_ADS_FINAL.copy(deep=True)\n# DF_PRED['Last_Updated'] = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n# DF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n#                     chunksize=None, reauth=False, if_exists='append', auth_local_webserver=False,\n#                     table_schema=None, location=None, progress_bar=True, credentials=None)\n\n# DF_PRED.to_gbq(CONFIGPARSER['SETTINGS']['BQ_IPL_LIVE_PREDICTIONS'], project_id=CONFIGPARSER['SETTINGS']['PROJECT_ID'],\n#                     chunksize=None, reauth=False, if_exists='replace', auth_local_webserver=False,\n#                     table_schema=None, location=None, progress_bar=True, credentials=None)\n\n# logging.info('Prediction Live path %s', CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'])\n# DF_ADS_FINAL.to_csv(CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_LIVE'], index=False)\n\n# YEAR_MONTH = datetime.now(timezone('US/Eastern')).strftime('%Y-%m')\n# CURRENT_DATE = datetime.now(timezone('US/Eastern')).strftime('%Y-%m-%d')\n# CURRENT_DATE_HOUR = datetime.now(timezone('US/Eastern')).strftime('%Y%m%d%H%M')\n# logging.info('Year Month in Eastern Time Zone %s', YEAR_MONTH)\n# logging.info('Current Month in Eastern Time Zone %s', CURRENT_DATE)\n# logging.info('Current Date & Hour in Eastern Time Zone %s', CURRENT_DATE_HOUR)\n# logging.info('\\n')\n\n# FILENAME = CONFIGPARSER['LOAD_AND_PREDICT']['PREDICTION_BACKUP'] + '{}/{}/TTR_predictions_{}.csv'.format(YEAR_MONTH, CURRENT_DATE, CURRENT_DATE_HOUR)\n# logging.info('Backup Storage Predictions Storage Path: %s', FILENAME)\n\n# DF_ADS_FINAL.to_csv(FILENAME, index=False)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}