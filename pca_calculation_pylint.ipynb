{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "'''\nAuthor - Mu Sigma\nUpdated: 4 Dec 2020\nVersion: 2\nTasks: PCA calculation, Number of Outages, Outage Duration, \nCustomer Quantity, Recovery Duration, Output Collation\nDescription: PCA calculation script\nEnvironment: Composer-0001\nRun-time environments: Pyspark,SparkR and python 3.7 callable\n'''\n\n# standard library imports\nimport logging\nfrom datetime import date, timedelta\nimport numpy as np\nimport pandas as pd\nfrom pandas.io import gbq\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n#Third party libraries\nimport subprocess\nimport ast\nfrom configparser import ConfigParser, ExtendedInterpolation\n\n# logging setup\n# Setup logs\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# read config file\nCONFIGPARSER = ConfigParser(interpolation=ExtendedInterpolation())\nCONFIGPARSER.read('/root/config_storm.ini')\nlogging.info('Config File Loaded')\nlogging.info('Config File Sections %s', CONFIGPARSER.sections())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Configuration setup \nWS_LOCATION = CONFIGPARSER['PCA_CALC']['weather_query']\nWS_LOCATION_2 = CONFIGPARSER['PCA_CALC']['weather_query_2']\nSTORM_DATA_PATH = CONFIGPARSER['PCA_CALC']['storm_id_level_data']\nOP_PATH = CONFIGPARSER['PCA_CALC']['pca_output_path']\nPROJECT_ID = CONFIGPARSER['PCA_CALC']['PROJECT_ID']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "CURRENT_DATE = date.today()\nlogging.info('Current Datetime %s \\n', CURRENT_DATE)\nYEAR_MONTH = pd.to_datetime(CURRENT_DATE).strftime('%Y-%m')\nlogging.info('Current Year Month %s \\n', YEAR_MONTH)\nTODAY = pd.to_datetime(CURRENT_DATE).strftime('%Y-%m-%d')\nlogging.info('Todays Date %s \\n', TODAY)\nTOMORROW1 = CURRENT_DATE + timedelta(1)\nlogging.info('Tomowrows Date %s \\n', TOMORROW1)\nDAYAFTER1 = CURRENT_DATE + timedelta(2)\nlogging.info('Day after tomowrows Date %s \\n', DAYAFTER1)\n\nYEAR_MONTH_1 = pd.to_datetime(TOMORROW1).strftime('%Y-%m')\nlogging.info('Tomorows Year-Month %s \\n', YEAR_MONTH_1)\nYEAR_MONTH_2 = pd.to_datetime(DAYAFTER1).strftime('%Y-%m')\nlogging.info('Day after tomorows Year-Month %s \\n', YEAR_MONTH_2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# CURRENT_DATE = date.today()\n# logging.info('Current Datetime %s \\n', CURRENT_DATE)\n# YEAR_MONTH = pd.to_datetime(CURRENT_DATE).strftime('%Y-%m')\n# logging.info('Current Year Month %s \\n', YEAR_MONTH)\n# TODAY = pd.to_datetime(CURRENT_DATE).strftime('%Y-%m-%d')\n# logging.info('Todays Date %s \\n', TODAY)\n# TOMORROW = CURRENT_DATE + timedelta(1)\n# logging.info('Tomowrows Date %s \\n', TOMORROW)\n# DAYAFTER = CURRENT_DATE + timedelta(2)\n# logging.info('Day after tomowrows Date %s \\n', DAYAFTER)\n\n# YEAR_MONTH_1 = pd.to_datetime(TOMORROW1).strftime('%Y-%m')\n# logging.info('Tomorows Year-Month %s \\n', YEAR_MONTH_1)\n# YEAR_MONTH_2 = pd.to_datetime(DAYAFTER1).strftime('%Y-%m')\n# logging.info('Day after tomorows Year-Month %s \\n', YEAR_MONTH_2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# PATH1 = \"gs://aes-datahub-0001-raw/Weather/weather_source/USA/Indianapolis/\"\\\n#         + YEAR_MONTH_1 + \"/forecast_data/\" + TODAY + \"/weathersource_daily_\" + TOMORROW + \".csv\"\n# logging.info('Forecasted weather data path 1 %s \\n', PATH1)\n# PATH2 = \"gs://aes-datahub-0001-raw/Weather/weather_source/USA/Indianapolis/\"\\\n#         + YEAR_MONTH_2 + \"/forecast_data/\" + TODAY + \"/weathersource_daily_\" + DAYAFTER + \".csv\"\n# logging.info('Forecasted weather data path 2 %s \\n', PATH2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Reading weather data for tomorrow\n#WS_LOCATION = \"SELECT * FROM aes-analytics-0002.weathersource_mds.forecast_day where Location LIKE 'IPL%' and timestamp = '{} 05:00:00' order by timestamp asc\"\n\nNEW_DATA = gbq.read_gbq(WS_LOCATION.format(TOMORROW1), project_id=PROJECT_ID)\nNEW_DATA.drop_duplicates(['timestamp', 'Location'], keep='last', inplace=True)\nNEW_DATA.reset_index(drop=True, inplace=True)\nprint(NEW_DATA.shape)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Reading weather data for day after tomorrow\nNEW_DATA2 = gbq.read_gbq(WS_LOCATION_2.format(DAYAFTER1), project_id=PROJECT_ID)\nNEW_DATA2.drop_duplicates(['timestamp', 'Location'], keep='last', inplace=True)\nNEW_DATA2.reset_index(drop=True, inplace=True)\nprint(NEW_DATA2.shape)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Reading the forecast files\n# NEW_DATA = pd.read_csv(PATH1, index_col=0)\n# NEW_DATA.reset_index(drop=True, inplace=True)\n# logging.info(NEW_DATA.shape)\n# NEW_DATA2 = pd.read_csv(PATH2, index_col=0)\n# NEW_DATA2.reset_index(drop=True, inplace=True)\n# logging.info(NEW_DATA2.shape)\n\n# Reading the storm profiles data data\n\nSTORM_DATA = pd.read_csv(STORM_DATA_PATH, index_col=0)\nlogging.info('Shape of Storm id Level Data %s \\n', STORM_DATA.shape)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def preprocess_data(new_data):\n    \"\"\"\n    Input - Weather Dataframe wtit raw columns\n    Output - Weather Dataframe with required columns\n    \"\"\"\n    # Dropping unnecessary columns and clening of Marker id's\n    new_data = new_data.drop(['Latitude', 'Longitude', 'timestampInit'], axis=1)\n    new_data['Location'] = new_data['Location'].str.replace(' ', '')\n\n    # Renaming Columns\n    new_data.rename(columns={\"timestamp\": \"Date\"}, inplace=True)\n\n    # Selecting required variables\n    new_data = new_data[['cldCvrAvg', 'cldCvrMax', 'cldCvrMin', 'dewPtAvg', 'dewPtMax', 'dewPtMin',\n                         'feelsLikeAvg', 'feelsLikeMax',\n                         'feelsLikeMin', 'heatIndexAvg', 'heatIndexMax', 'heatIndexMin',\n                         'mslPresAvg', 'mslPresMax', 'mslPresMin', 'precip', 'radSolarAvg',\n                         'radSolarMax', 'radSolarTot', 'relHumAvg', 'relHumMax', 'relHumMin',\n                         'sfcPresAvg', 'sfcPresMax', 'sfcPresMin',\n                         'snowfall', 'spcHumAvg', 'spcHumMax', 'spcHumMin', 'tempAvg',\n                         'tempMax', 'tempMin', 'Date', 'wetBulbAvg', 'wetBulbMax', 'wetBulbMin',\n                         'windChillAvg', 'windChillMax', 'windChillMin', 'windDir100mAvg',\n                         'windDir80mAvg', 'windDirAvg', 'windSpd100mAvg', 'windSpd100mMax',\n                         'windSpd100mMin', 'windSpd80mAvg', 'windSpd80mMax', 'windSpd80mMin',\n                         'windSpdAvg', 'windSpdMax', 'windSpdMin', 'Location']]\n\n\n    # Converting to day level date\n    new_data['Date'] = pd.to_datetime(new_data['Date']).dt.strftime('%Y-%m-%d')\n\n    return new_data\n\ndef separate_different_markers(new_data):\n    \"\"\"\n    Input - Dataframe with 20 different Marker Locations\n    Output - 20 different Marker Level \n    \"\"\"\n    req_cols = ['Date', 'cldCvrAvg', 'cldCvrMax', 'cldCvrMin', 'dewPtAvg', 'dewPtMax',\n                'dewPtMin', 'feelsLikeAvg', 'feelsLikeMax',\n                'feelsLikeMin', 'heatIndexAvg', 'heatIndexMax', 'heatIndexMin',\n                'mslPresAvg', 'mslPresMax', 'mslPresMin', 'precip', 'radSolarAvg',\n                'radSolarMax', 'radSolarTot', 'relHumAvg', 'relHumMax', 'relHumMin', 'sfcPresAvg',\n                'sfcPresMax', 'sfcPresMin', 'snowfall', 'spcHumAvg', 'spcHumMax', 'spcHumMin',\n                'tempAvg', 'tempMax', 'tempMin', 'wetBulbAvg', 'wetBulbMax', 'wetBulbMin',\n                'windChillAvg', 'windChillMax', 'windChillMin', 'windDir100mAvg',\n                'windDir80mAvg', 'windDirAvg', 'windSpd100mAvg', 'windSpd100mMax',\n                'windSpd100mMin', 'windSpd80mAvg', 'windSpd80mMax', 'windSpd80mMin',\n                'windSpdAvg', 'windSpdMax', 'windSpdMin']\n\n#     marker1 = new_data[new_data.Location == 'Marker1'][req_cols]\n#     marker2 = new_data[new_data.Location == 'Marker2'][req_cols]\n#     marker3 = new_data[new_data.Location == 'Marker3'][req_cols]\n#     marker4 = new_data[new_data.Location == 'Marker4'][req_cols]\n#     marker5 = new_data[new_data.Location == 'Marker5'][req_cols]\n#     marker6 = new_data[new_data.Location == 'Marker6'][req_cols]\n#     marker7 = new_data[new_data.Location == 'Marker7'][req_cols]\n#     marker8 = new_data[new_data.Location == 'Marker8'][req_cols]\n#     marker9 = new_data[new_data.Location == 'Marker9'][req_cols]\n#     marker10 = new_data[new_data.Location == 'Marker10'][req_cols]\n#     marker11 = new_data[new_data.Location == 'Marker11'][req_cols]\n#     marker12 = new_data[new_data.Location == 'Marker12'][req_cols]\n#     marker13 = new_data[new_data.Location == 'Marker13'][req_cols]\n#     marker14 = new_data[new_data.Location == 'Marker14'][req_cols]\n#     marker15 = new_data[new_data.Location == 'Marker15'][req_cols]\n#     marker16 = new_data[new_data.Location == 'Marker16'][req_cols]\n#     marker17 = new_data[new_data.Location == 'Marker17'][req_cols]\n#     marker18 = new_data[new_data.Location == 'Marker18'][req_cols]\n#     marker19 = new_data[new_data.Location == 'Marker19'][req_cols]\n#     marker20 = new_data[new_data.Location == 'Marker20'][req_cols]\n\n\n    marker1 = new_data[new_data.Location == 'IPL_Marker1'][req_cols]\n    marker2 = new_data[new_data.Location == 'IPL_Marker2'][req_cols]\n    marker3 = new_data[new_data.Location == 'IPL_Marker3'][req_cols]\n    marker4 = new_data[new_data.Location == 'IPL_Marker4'][req_cols]\n    marker5 = new_data[new_data.Location == 'IPL_Marker5'][req_cols]\n    marker6 = new_data[new_data.Location == 'IPL_Marker6'][req_cols]\n    marker7 = new_data[new_data.Location == 'IPL_Marker7'][req_cols]\n    marker8 = new_data[new_data.Location == 'IPL_Marker8'][req_cols]\n    marker9 = new_data[new_data.Location == 'IPL_Marker9'][req_cols]\n    marker10 = new_data[new_data.Location == 'IPL_Marker10'][req_cols]\n    marker11 = new_data[new_data.Location == 'IPL_Marker11'][req_cols]\n    marker12 = new_data[new_data.Location == 'IPL_Marker12'][req_cols]\n    marker13 = new_data[new_data.Location == 'IPL_Marker13'][req_cols]\n    marker14 = new_data[new_data.Location == 'IPL_Marker14'][req_cols]\n    marker15 = new_data[new_data.Location == 'IPL_Marker15'][req_cols]\n    marker16 = new_data[new_data.Location == 'IPL_Marker16'][req_cols]\n    marker17 = new_data[new_data.Location == 'IPL_Marker17'][req_cols]\n    marker18 = new_data[new_data.Location == 'IPL_Marker18'][req_cols]\n    marker19 = new_data[new_data.Location == 'IPL_Marker19'][req_cols]\n    marker20 = new_data[new_data.Location == 'IPL_Marker20'][req_cols]\n\n    return (marker1, marker2, marker3, marker4, marker5, marker6, marker7, marker8, marker9,\n            marker10, marker11, marker12, marker13, marker14, marker15, marker16, marker17,\n            marker18, marker19, marker20)\n\ndef rename_markers(marker1, marker2, marker3, marker4, marker5, marker6, marker7, marker8, marker9,\n                   marker10, marker11, marker12, marker13, marker14, marker15, marker16, marker17,\n                   marker18, marker19, marker20):\n    \"\"\"\n    Input - Marker Level dataframe\n    Output - Renamed dataframe with Marker ID's\n    \"\"\"\n    \n    # marker's name\n    location = ['MARKER1', 'MARKER2', 'MARKER3', 'MARKER4', 'MARKER5', 'MARKER6', 'MARKER7',\n                'MARKER8', 'MARKER9', 'MARKER10',\n                'MARKER11', 'MARKER12', 'MARKER13', 'MARKER14', 'MARKER15', 'MARKER16',\n                'MARKER17', 'MARKER18', 'MARKER19', 'MARKER20']\n\n    marker_name = [marker1, marker2, marker3, marker4, marker5, marker6, marker7, marker8, marker9,\n                   marker10, marker11, marker12, marker13, marker14, marker15, marker16, marker17,\n                   marker18, marker19, marker20]\n\n\n    for i, j in enumerate(marker_name):\n        j.rename(columns={\"cldCvrAvg\": location[i]+\"_cldCvrAvg\",\n                          \"cldCvrMax\": location[i]+\"_cldCvrMax\",\n                          \"cldCvrMin\": location[i]+\"_cldCvrMin\",\n\n                          \"dewPtAvg\": location[i]+\"_dewPtAvg\",\n                          \"dewPtMax\": location[i]+\"_dewPtMax\",\n                          \"dewPtMin\": location[i]+\"_dewPtMin\",\n\n                          \"feelsLikeAvg\": location[i]+\"_feelsLikeAvg\",\n                          \"feelsLikeMax\": location[i]+\"_feelsLikeMax\",\n                          \"feelsLikeMin\": location[i]+\"_feelsLikeMin\",\n\n                          \"heatIndexAvg\": location[i]+\"_heatIndexAvg\",\n                          \"heatIndexMax\": location[i]+\"_heatIndexMax\",\n                          \"heatIndexMin\": location[i]+\"_heatIndexMin\",\n\n                          \"mslPresAvg\": location[i]+\"_mslPresAvg\",\n                          \"mslPresMax\": location[i]+\"_mslPresMax\",\n                          \"mslPresMin\": location[i]+\"_mslPresMin\",\n\n                          \"precip\": location[i]+\"_precip\",\n\n                          \"radSolarAvg\": location[i]+\"_radSolarAvg\",\n                          \"radSolarMax\": location[i]+\"_radSolarMax\",\n\n                          \"radSolarTot\": location[i]+\"_radSolarTot\",\n\n                          \"relHumAvg\": location[i]+\"_relHumAvg\",\n                          \"relHumMax\": location[i]+\"_relHumMax\",\n                          \"relHumMin\": location[i]+\"_relHumMin\",\n\n                          \"sfcPresAvg\": location[i]+\"_sfcPresAvg\",\n                          \"sfcPresMax\": location[i]+\"_sfcPresMax\",\n                          \"sfcPresMin\": location[i]+\"_sfcPresMin\",\n\n                          \"snowfall\": location[i]+\"_snowfall\",\n\n                          \"spcHumAvg\": location[i]+\"_spcHumAvg\",\n                          \"spcHumMax\": location[i]+\"_spcHumMax\",\n                          \"spcHumMin\": location[i]+\"_spcHumMin\",\n\n                          \"tempAvg\": location[i]+\"_tempAvg\",\n                          \"tempMin\": location[i]+\"_tempMin\",\n                          \"tempMax\": location[i]+\"_tempMax\",\n\n                          \"wetBulbAvg\": location[i]+\"_wetBulbAvg\",\n                          \"wetBulbMax\": location[i]+\"_wetBulbMax\",\n                          \"wetBulbMin\": location[i]+\"_wetBulbMin\",\n\n                          \"windChillAvg\": location[i]+\"_windChillAvg\",\n                          \"windChillMax\": location[i]+\"_windChillMax\",\n                          \"windChillMin\": location[i]+\"_windChillMin\",\n\n                          \"windDir100mAvg\": location[i]+\"_windDir100mAvg\",\n                          \"windDir80mAvg\": location[i]+\"_windDir80mAvg\",\n                          \"windDirAvg\": location[i]+\"_windDirAvg\",\n\n                          \"windSpd100mAvg\": location[i]+\"_windSpd100mAvg\",\n                          \"windSpd100mMax\": location[i]+\"_windSpd100mMax\",\n                          \"windSpd100mMin\": location[i]+\"_windSpd100mMin\",\n\n                          \"windSpd80mAvg\": location[i]+\"_windSpd80mAvg\",\n                          \"windSpd80mMax\": location[i]+\"_windSpd80mMax\",\n                          \"windSpd80mMin\": location[i]+\"_windSpd80mMin\",\n\n                          \"windSpdAvg\": location[i]+\"_windSpdAvg\",\n                          \"windSpdMax\": location[i]+\"_windSpdMax\",\n                          \"windSpdMin\": location[i]+\"_windSpdMin\",\n                          }, inplace=True)\n\n    return (marker1, marker2, marker3, marker4, marker5, marker6, marker7, marker8, marker9,\n            marker10, marker11, marker12, marker13, marker14, marker15, marker16, marker17,\n            marker18, marker19, marker20)\n\ndef merge_markers_dataframe(marker1, marker2, marker3, marker4, marker5, marker6, marker7, marker8,\n                            marker9, marker10,\n                            marker11, marker12, marker13, marker14, marker15, marker16, marker17,\n                            marker18, marker19, marker20):\n    \"\"\"\n    Input - 20 different weather dataframe at marker level\n    Output - Merged final analytical dataframe\n    \"\"\"\n    final_ads_1 = pd.merge(marker1, marker2, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker3, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker4, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker5, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker6, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker7, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker8, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker9, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker10, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker11, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker12, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker13, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker14, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker15, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker16, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker17, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker18, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker19, how='left', on=['Date'])\n    final_ads_1 = pd.merge(final_ads_1, marker20, how='left', on=['Date'])\n\n    return final_ads_1\n\n# Dummified data\nDF1 = preprocess_data(NEW_DATA)\nMR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20 = separate_different_markers(DF1)\nMR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20 = rename_markers(\n    MR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20)\nFINAL_DF1 = merge_markers_dataframe(MR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20)\n\nDF2 = preprocess_data(NEW_DATA2)\nMR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20 = separate_different_markers(DF2)\nMR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20 = rename_markers(\n    MR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20)\nFINAL_DF2 = merge_markers_dataframe(MR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8, MR9, MR10, MR11, MR12, MR13, MR14, MR15, MR16, MR17, MR18, MR19, MR20)\n\n# PCA on Storm Data\nSTORM_DATA = STORM_DATA.loc[:, ~STORM_DATA.columns.str.contains('^Unnamed')]\nSTORM_DATA = STORM_DATA.loc[:, ~STORM_DATA.columns.str.contains('^_c0')]\nSTORM_DATA_DIMS = STORM_DATA.copy(deep=True)\nSTORM_DATA_DIMS.drop(['MAJ_OTG_ID', 'Date', 'OUTAGES', 'CUST_QTY', 'STORM_DURATION',\n                      'OUTAGED_RECOVERY', 'MARKER1_radSolarMin', 'MARKER2_radSolarMin',\n                      'MARKER3_radSolarMin', 'MARKER4_radSolarMin', 'MARKER5_radSolarMin',\n                      'MARKER6_radSolarMin', 'MARKER7_radSolarMin',\n                      'MARKER8_radSolarMin', 'MARKER9_radSolarMin', 'MARKER10_radSolarMin',\n                      'MARKER11_radSolarMin', 'MARKER12_radSolarMin',\n                      'MARKER13_radSolarMin', 'MARKER14_radSolarMin', 'MARKER15_radSolarMin',\n                      'MARKER16_radSolarMin', 'MARKER17_radSolarMin',\n                      'MARKER18_radSolarMin', 'MARKER19_radSolarMin',\n                      'MARKER20_radSolarMin'], axis=1, inplace=True)\n\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Calculating mean and standard deviation\nMEANDATA = STORM_DATA_DIMS.mean(axis=0, skipna=True)\nSTDDATA = STORM_DATA_DIMS.std(axis=0, skipna=True)\n\n# Storing required features\nFEATURES = list(STORM_DATA_DIMS.columns)\n\n# Setting standar scaler\nSCALER = StandardScaler()\n\n# Fitting on dataset\nlogging.info(STORM_DATA_DIMS.shape)\nSCALER.fit(STORM_DATA_DIMS)\n\n# Transforming dataframe\nSCALED_DATA = SCALER.transform(STORM_DATA_DIMS)\n\n# Data after scaling\nSTORM_DATA_DIMS_SCALED = pd.DataFrame(SCALED_DATA, columns=FEATURES)\nround(STORM_DATA_DIMS_SCALED.describe(), 2)\n\n# PCA covariance matrix calculation\nCOVAR_MATRIX = PCA()\n\n# Fitting Scaled data into covariance matrix\nCOVAR_MATRIX.fit(SCALED_DATA)\n\n# Crosscheck values whether variance is 88.9 or not\nVARIANCE = COVAR_MATRIX.explained_variance_ratio_\nVAR = np.cumsum(np.round(COVAR_MATRIX.explained_variance_ratio_, decimals=3)*100)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def pcafunc(dff):\n    \"\"\"\n    Input - Preprocessed Dataframe\n    Output - PCA calculated dataframe\n    \"\"\"\n    # Scaling the new data\n    for x in range(1, 1000):\n        colname = dff.columns[x]\n        val_a = MEANDATA.loc[colname]\n        val_b = STDDATA.loc[colname]\n        val_d = dff.at[0, colname]\n        val_e = (val_d-val_a)/val_b\n        dff[colname] = dff[colname].replace([val_d], val_e)\n\n    # storing date and dropping date column\n    timestamp = dff.at[0, 'Date']\n    dff = dff.drop(['Date'], axis=1)\n    logging.info(dff.shape)\n\n    # PCA on new data\n    newdata_transformed = COVAR_MATRIX.transform(dff)\n    \n    # Transforming to a dataframe\n    newdata_transformed = pd.DataFrame(newdata_transformed)\n    newdata_transformed = newdata_transformed.iloc[:, 0:7]\n    newdata_transformed.columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7']\n    newdata_transformed.head()\n\n    # Adding date column in data\n    newdata_transform = newdata_transformed\n    newdata_transform['Date'] = timestamp\n    \n    return newdata_transform\n\nlogging.info(\"PCA FUNCTION IS CREATED\")\n\nPCA1 = pcafunc(FINAL_DF1)\nPCA2 = pcafunc(FINAL_DF2)\n\n\nOP_PATH1 = OP_PATH + YEAR_MONTH + '/' + TODAY + '/' + 'PCA1.csv'\nOP_PATH2 = OP_PATH + YEAR_MONTH + '/' + TODAY + '/' + 'PCA2.csv'"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "PCA1.to_csv(OP_PATH1, index=False)\nPCA2.to_csv(OP_PATH2, index=False)\nlogging.info(\"**PCA calculation task completed**\")\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}